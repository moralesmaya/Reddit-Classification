{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing language across Feminist and Men's Rights subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayamorales/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#imports  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression,  LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data \n",
    "\n",
    "df = pd.read_csv('data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 , CV and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['text', 'title']\n",
    "X = df[['title']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(max_df=1, max_features = 5000,\n",
    "                       stop_words = 'english', lowercase = True, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvec = cvec.fit_transform(X_train['title'])\n",
    "\n",
    "pd.DataFrame(X_train_cvec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "X_test_cvec = cvec.transform(X_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert X test data\n",
    "pd.DataFrame(X_test_cvec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "#Instantiate Model\n",
    "nb = MultinomialNB(alpha= 1.0)\n",
    "\n",
    "#fit model \n",
    "nb.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our predictions\n",
    "pred = nb.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train score:0.7995555555555556  Test score:0.5906666666666667\n"
     ]
    }
   ],
   "source": [
    "#93 and 68\n",
    "\n",
    "print(f' Train score:{nb.score(X_train_cvec, y_train)}  Test score:{nb.score(X_test_cvec, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MultiNomial w/ CV Train': 0.7995555555555556,\n",
       " 'MultiNomial w/ CV Test': 0.5906666666666667}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dictionary to keep track of scores. \n",
    "\n",
    "model_scores_1 = {\n",
    "    'MultiNomial w/ CV Train': nb.score(X_train_cvec, y_train),\n",
    "    'MultiNomial w/ CV Test':nb.score(X_test_cvec, y_test)\n",
    "}\n",
    "\n",
    "model_scores_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2, CV and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline\n",
    "\n",
    "pipe = Pipeline([('cv', CountVectorizer()), ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune Parameters \n",
    "\n",
    "params_grid_cv = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2)],\n",
    "    'cv__max_df' : [1.0, 0.95],\n",
    "    'cv__min_df' : [1, 2],\n",
    "    'cv__max_features' : [ 5000, 8000],\n",
    "    'lr__C' : [1, .05],\n",
    "    'lr__penalty' : ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cv = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid = params_grid_cv, #parameters values we are searching \n",
    "                  cv = 5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__stop_words: english\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__max_df: 1.0\n",
      "cv__min_df: 1\n",
      "cv__max_features: 8000\n",
      "lr__C: 1\n",
      "lr__penalty: l2\n",
      "Train score: 0.6928888888888889 Test score 0.696\n"
     ]
    }
   ],
   "source": [
    "#Fit and Score model \n",
    "\n",
    "gs_cv.fit(X_train['title'], y_train)\n",
    "score_train = gs_cv.best_score_\n",
    "score_test = gs_cv.score(X_test['title'], y_test)\n",
    "params_train = gs_cv.best_params_\n",
    "\n",
    "for k in params_grid_cv:\n",
    "    print(\"{}: {}\".format(k,params_train[k]))\n",
    "\n",
    "print(\"Train score: {} Test score {}\".format(score_train, score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coefs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>false</th>\n",
       "      <td>-1.458918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ex</th>\n",
       "      <td>-1.347470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boys</th>\n",
       "      <td>-1.286297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girls</th>\n",
       "      <td>1.281804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circumcision</th>\n",
       "      <td>-1.280766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misandry</th>\n",
       "      <td>-1.230945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feminism women</th>\n",
       "      <td>-1.220608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charge</th>\n",
       "      <td>1.178651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mra</th>\n",
       "      <td>-1.174353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men</th>\n",
       "      <td>-1.120018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    vals\n",
       "coefs                   \n",
       "false          -1.458918\n",
       "ex             -1.347470\n",
       "boys           -1.286297\n",
       "girls           1.281804\n",
       "circumcision   -1.280766\n",
       "misandry       -1.230945\n",
       "feminism women -1.220608\n",
       "charge          1.178651\n",
       "mra            -1.174353\n",
       "men            -1.120018"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most important words\n",
    "\n",
    "coef_names = gs_cv.best_estimator_.named_steps['cv'].get_feature_names()\n",
    "coef_vals = gs_cv.best_estimator_.named_steps['lr'].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame ({\n",
    "    'coefs' : coef_names,\n",
    "    'vals' : coef_vals\n",
    "}).set_index('coefs')\n",
    "\n",
    "coef_df.reindex(coef_df['vals'].abs().sort_values(ascending=False).index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train score:0.9408888888888889  Test score:0.696\n"
     ]
    }
   ],
   "source": [
    "#94 and 69\n",
    "\n",
    "print(f\" Train score:{gs.score(X_train['title'], y_train)}  Test score:{gs.score(X_test['title'], y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MultiNomial w/ CV Train': 0.7995555555555556,\n",
       " 'MultiNomial w/ CV Test': 0.5906666666666667,\n",
       " 'LogReg and CV Train': 0.9408888888888889,\n",
       " 'LogReg and CV Test': 0.696}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding to my Dictionary\n",
    "\n",
    "model_scores_1['LogReg and CV Train'] = gs.score(X_train['title'], y_train)\n",
    "model_scores_1['LogReg and CV Test'] = gs.score(X_test['title'], y_test)\n",
    "model_scores_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a function to better tune parameters and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipline(items, use_params, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    #models  \n",
    "    pipe_items = {\n",
    "        \n",
    "        #vectorizers \n",
    "        'cv': CountVectorizer(),\n",
    "        'tv': TfidfVectorizer(),\n",
    "        \n",
    "        #scaling\n",
    "        'ss' : StandardScaler(),\n",
    "        'pf' : PolynomialFeatures(),\n",
    "        \n",
    "        #Classifications\n",
    "        'lr' : LogisticRegression(),\n",
    "        'bnb' : BernoulliNB(),\n",
    "        'mnb' : MultinomialNB(),\n",
    "        'rf' : RandomForestClassifier(),\n",
    "        'ab' : AdaBoostClassifier(),\n",
    "        'svc' : SVC(),\n",
    "        'knn' : KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    # Pipeline Parameters\n",
    "    param_items = {\n",
    "        'cv' : {\n",
    "            'cv__stop_words' : [None, 'english'],\n",
    "            'cv__ngram_range' : [(1,1), (1,2)],\n",
    "            'cv__max_df' : [1.0, 0.95],\n",
    "            'cv__min_df' : [1],\n",
    "            'cv__max_features' : [5000, 8000]\n",
    "        },\n",
    "        'tv' : {\n",
    "            'tv__stop_words' : [None, 'english'],\n",
    "            'tv__ngram_range' : [(1,1), (1,2)],\n",
    "            'tv__max_df' : [1.0, 0.95],\n",
    "            'tv__min_df' : [1, 2],\n",
    "            'tv__max_features' : [5000, 8000]\n",
    "        },\n",
    "        'ss' : {\n",
    "            'ss__with_mean' : [False]\n",
    "        },\n",
    "        'pf' : {\n",
    "            'pf__degree' : [2]\n",
    "        },\n",
    "        'lr' : {\n",
    "            'lr__C' : [1, .05],\n",
    "            'lr__penalty' : ['l2']\n",
    "        },\n",
    "        'bnb' : {\n",
    "            'bnb__alpha' : [1.0, 1.5, 1.8, 2.0]\n",
    "        },\n",
    "        'mnb' : {\n",
    "            'mnb__alpha' : [0.8, 1.0, 1.2]\n",
    "        },\n",
    "        'rf' : {\n",
    "            'rf__n_estimators' : [8, 10, 15]\n",
    "        },\n",
    "        'ab' : {\n",
    "            'ab__n_estimators' : [75, 50, 125]\n",
    "        },\n",
    "        'svc' : {\n",
    "            'svc__kernel' : ['linear','poly']\n",
    "        },\n",
    "        'knn' : {\n",
    "            'knn__n_neighbors' : [25,35,45]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the parameters for GridSearch\n",
    "    params = dict()\n",
    "    if use_params:\n",
    "        for i in items:\n",
    "            for p in param_items[i]:\n",
    "                params[p] = param_items[i][p]\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe_list = [(i,pipe_items[i]) for i in items]\n",
    "    print(\"Using:\")\n",
    "    for p in pipe_list:\n",
    "        print(\"\\t\" + str(p[1]).split('(')[0])\n",
    "    pipe = Pipeline(pipe_list)\n",
    "\n",
    "    # Grid search\n",
    "    gs = GridSearchCV(pipe, param_grid=params, verbose=1)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # Print the results\n",
    "    train_params = gs.best_params_\n",
    "    train_score = gs.best_score_\n",
    "    y_test_hat = gs.predict(X_test)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "\n",
    "    for k in train_params:\n",
    "        print(\"{}: {}\".format(k,train_params[k]))\n",
    "\n",
    "    print(\"Train score: {} Test score {}\".format(train_score, test_score))\n",
    "    print(\"\")\n",
    "\n",
    "    return train_score, test_score, y_test_hat, train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\tCountVectorizer\n",
      "\tLogisticRegression\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:   20.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__max_df: 1.0\n",
      "cv__max_features: 8000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__stop_words: english\n",
      "lr__C: 1\n",
      "lr__penalty: l2\n",
      "Train score: 0.6928888888888889 Test score 0.696\n",
      "\n",
      "Using:\n",
      "\tCountVectorizer\n",
      "\tBernoulliNB\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   28.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnb__alpha: 1.0\n",
      "cv__max_df: 1.0\n",
      "cv__max_features: 5000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 1)\n",
      "cv__stop_words: english\n",
      "Train score: 0.6635555555555556 Test score 0.6706666666666666\n",
      "\n",
      "Using:\n",
      "\tCountVectorizer\n",
      "\tMultinomialNB\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__max_df: 1.0\n",
      "cv__max_features: 8000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__stop_words: english\n",
      "mnb__alpha: 1.0\n",
      "Train score: 0.6799999999999999 Test score 0.684\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tLogisticRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:   37.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr__C: 1\n",
      "lr__penalty: l2\n",
      "tv__max_df: 1.0\n",
      "tv__max_features: 5000\n",
      "tv__min_df: 1\n",
      "tv__ngram_range: (1, 2)\n",
      "tv__stop_words: english\n",
      "Train score: 0.6942222222222222 Test score 0.692\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tBernoulliNB\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 640 out of 640 | elapsed:   56.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnb__alpha: 1.0\n",
      "tv__max_df: 1.0\n",
      "tv__max_features: 5000\n",
      "tv__min_df: 2\n",
      "tv__ngram_range: (1, 1)\n",
      "tv__stop_words: english\n",
      "Train score: 0.6671111111111111 Test score 0.672\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tMultinomialNB\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "mnb__alpha: 1.2\n",
      "tv__max_df: 1.0\n",
      "tv__max_features: 5000\n",
      "tv__min_df: 2\n",
      "tv__ngram_range: (1, 2)\n",
      "tv__stop_words: english\n",
      "Train score: 0.6764444444444444 Test score 0.6813333333333333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:   45.9s finished\n"
     ]
    }
   ],
   "source": [
    "#Decide what to put into the pipline, grid searh, and save the \"best\" for each grid search\n",
    "use_params = True\n",
    "vects = ['cv','tv']\n",
    "models = ['lr','bnb', 'mnb','rf','ab','svc','knn']\n",
    "other = ['pf','ss']\n",
    "\n",
    "\n",
    "vects = ['cv','tv']\n",
    "models = ['lr','bnb', 'mnb']\n",
    "other = []\n",
    "\n",
    "model_scores_2 = {}\n",
    "idx = 0\n",
    "\n",
    "for v in vects:\n",
    "    for i in range(len(other)+1):\n",
    "        for o in list(combinations(other, i)):\n",
    "            for m in models:\n",
    "                idx += 1\n",
    "                pipe_items = [v]\n",
    "                pipe_items.extend(list(o))\n",
    "                pipe_items.append(m)\n",
    "                [train_score, test_score, y_test_hat, best_params] = make_pipline(pipe_items, use_params, \n",
    "                                                                        X_train['title'], X_test['title'], \n",
    "                                                                        y_train, y_test)\n",
    "                model_scores_2[idx] = {'vectorizer' : v, 'model': m, 'features': list(o),\n",
    "                                    'train_score': train_score, 'test_score': test_score, \n",
    "                                    'best_params': best_params, 'y_test_hat' : y_test_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\tCountVectorizer\n",
      "\tSVC\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__max_df: 1.0\n",
      "cv__max_features: 5000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__stop_words: None\n",
      "svc__kernel: linear\n",
      "Train score: 0.6635555555555556 Test score 0.6826666666666666\n",
      "\n",
      "Using:\n",
      "\tCountVectorizer\n",
      "\tKNeighborsClassifier\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   27.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__max_df: 1.0\n",
      "cv__max_features: 5000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__stop_words: None\n",
      "knn__n_neighbors: 25\n",
      "Train score: 0.5142222222222222 Test score 0.504\n",
      "\n",
      "Using:\n",
      "\tCountVectorizer\n",
      "\tRandomForestClassifier\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:   52.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv__max_df: 0.95\n",
      "cv__max_features: 8000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 1)\n",
      "cv__stop_words: None\n",
      "rf__n_estimators: 15\n",
      "Train score: 0.6662222222222222 Test score 0.6906666666666667\n",
      "\n",
      "Using:\n",
      "\tCountVectorizer\n",
      "\tAdaBoostClassifier\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab__n_estimators: 125\n",
      "cv__max_df: 1.0\n",
      "cv__max_features: 8000\n",
      "cv__min_df: 1\n",
      "cv__ngram_range: (1, 2)\n",
      "cv__stop_words: english\n",
      "Train score: 0.6653333333333333 Test score 0.644\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tSVC\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc__kernel: linear\n",
      "tv__max_df: 1.0\n",
      "tv__max_features: 8000\n",
      "tv__min_df: 1\n",
      "tv__ngram_range: (1, 2)\n",
      "tv__stop_words: english\n",
      "Train score: 0.7013333333333333 Test score 0.7146666666666667\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tKNeighborsClassifier\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:   56.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn__n_neighbors: 45\n",
      "tv__max_df: 1.0\n",
      "tv__max_features: 5000\n",
      "tv__min_df: 1\n",
      "tv__ngram_range: (1, 1)\n",
      "tv__stop_words: None\n",
      "Train score: 0.5506666666666666 Test score 0.5133333333333333\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tRandomForestClassifier\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf__n_estimators: 8\n",
      "tv__max_df: 0.95\n",
      "tv__max_features: 8000\n",
      "tv__min_df: 2\n",
      "tv__ngram_range: (1, 1)\n",
      "tv__stop_words: english\n",
      "Train score: 0.6813333333333333 Test score 0.6546666666666666\n",
      "\n",
      "Using:\n",
      "\tTfidfVectorizer\n",
      "\tAdaBoostClassifier\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 480 out of 480 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab__n_estimators: 125\n",
      "tv__max_df: 0.95\n",
      "tv__max_features: 5000\n",
      "tv__min_df: 2\n",
      "tv__ngram_range: (1, 2)\n",
      "tv__stop_words: english\n",
      "Train score: 0.6564444444444444 Test score 0.648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_params = True\n",
    "vects = ['cv','tv']\n",
    "models = ['lr','bnb','mnb','rf','ab','svc','knn']\n",
    "other = ['pf','ss']\n",
    "\n",
    "\n",
    "vects = ['cv','tv']\n",
    "models = ['svc','knn', 'rf', 'ab']\n",
    "other = []\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for v in vects:\n",
    "    for i in range(len(other)+1):\n",
    "        for o in list(combinations(other, i)):\n",
    "            for m in models:\n",
    "                idx += 1\n",
    "                pipe_items = [v]\n",
    "                pipe_items.extend(list(o))\n",
    "                pipe_items.append(m)\n",
    "                [train_score, test_score, y_test_hat, best_params] = make_pipline(pipe_items, use_params, \n",
    "                                                                        X_train['title'], X_test['title'], \n",
    "                                                                        y_train, y_test)\n",
    "                model_scores[idx] = {'vectorizer' : v, 'model': m, 'features': list(o),\n",
    "                                    'train_score': train_score, 'test_score': test_score, \n",
    "                                    'best_params': best_params, 'y_test_hat' : y_test_hat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest, model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvec = cvec.transform(X_train['title'])\n",
    "X_test_cvec = cvec.transform(X_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate Model \n",
    "tree = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Fit Model\n",
    "tree.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train score: 0.7995555555555556 Test Score: 0.556\n"
     ]
    }
   ],
   "source": [
    "#score model\n",
    "\n",
    "print(f\" Train score: {tree.score(X_train_cvec, y_train)} Test Score: {tree.score(X_test_cvec, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MultiNomial w/ CV Train': 0.7995555555555556,\n",
       " 'MultiNomial w/ CV Test': 0.5906666666666667,\n",
       " 'LogReg and CV Train': 0.9408888888888889,\n",
       " 'LogReg and CV Test': 0.696,\n",
       " 'DecisionTree and TFDIF Train': 0.7995555555555556,\n",
       " 'DecisionTree and TFDIF Test': 0.556}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add scores to dictionary\n",
    "\n",
    "model_scores_1['DecisionTree and TFDIF Train'] = tree.score(X_train_cvec, y_train)\n",
    "model_scores_1['DecisionTree and TFDIF Test'] = tree.score(X_test_cvec, y_test)\n",
    "model_scores_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Step, bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(), random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate BaggingClassifier.\n",
    "bag = BaggingClassifier(base_estimator = LogisticRegression(),random_state = 42)\n",
    "\n",
    "# Fit BaggingClassifier.\n",
    "bag.fit(X_train_cvec, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train score: 0.7986666666666666 Test Score: 0.5893333333333334\n"
     ]
    }
   ],
   "source": [
    "# Score BaggingClassifier.\n",
    "print(f\" Train score: {bag.score(X_train_cvec, y_train)} Test Score: {bag.score(X_test_cvec, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'vectorizer': 'cv',\n",
       "  'model': 'svc',\n",
       "  'features': [],\n",
       "  'train_score': 0.6635555555555556,\n",
       "  'test_score': 0.6826666666666666,\n",
       "  'best_params': {'cv__max_df': 1.0,\n",
       "   'cv__max_features': 5000,\n",
       "   'cv__min_df': 1,\n",
       "   'cv__ngram_range': (1, 2),\n",
       "   'cv__stop_words': None,\n",
       "   'svc__kernel': 'linear'},\n",
       "  'y_test_hat': array([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "         1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1])},\n",
       " 2: {'vectorizer': 'cv',\n",
       "  'model': 'knn',\n",
       "  'features': [],\n",
       "  'train_score': 0.5142222222222222,\n",
       "  'test_score': 0.504,\n",
       "  'best_params': {'cv__max_df': 1.0,\n",
       "   'cv__max_features': 5000,\n",
       "   'cv__min_df': 1,\n",
       "   'cv__ngram_range': (1, 2),\n",
       "   'cv__stop_words': None,\n",
       "   'knn__n_neighbors': 25},\n",
       "  'y_test_hat': array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])},\n",
       " 3: {'vectorizer': 'cv',\n",
       "  'model': 'rf',\n",
       "  'features': [],\n",
       "  'train_score': 0.6662222222222222,\n",
       "  'test_score': 0.6906666666666667,\n",
       "  'best_params': {'cv__max_df': 0.95,\n",
       "   'cv__max_features': 8000,\n",
       "   'cv__min_df': 1,\n",
       "   'cv__ngram_range': (1, 1),\n",
       "   'cv__stop_words': None,\n",
       "   'rf__n_estimators': 15},\n",
       "  'y_test_hat': array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         0, 1])},\n",
       " 4: {'vectorizer': 'cv',\n",
       "  'model': 'ab',\n",
       "  'features': [],\n",
       "  'train_score': 0.6653333333333333,\n",
       "  'test_score': 0.644,\n",
       "  'best_params': {'ab__n_estimators': 125,\n",
       "   'cv__max_df': 1.0,\n",
       "   'cv__max_features': 8000,\n",
       "   'cv__min_df': 1,\n",
       "   'cv__ngram_range': (1, 2),\n",
       "   'cv__stop_words': 'english'},\n",
       "  'y_test_hat': array([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])},\n",
       " 5: {'vectorizer': 'tv',\n",
       "  'model': 'svc',\n",
       "  'features': [],\n",
       "  'train_score': 0.7013333333333333,\n",
       "  'test_score': 0.7146666666666667,\n",
       "  'best_params': {'svc__kernel': 'linear',\n",
       "   'tv__max_df': 1.0,\n",
       "   'tv__max_features': 8000,\n",
       "   'tv__min_df': 1,\n",
       "   'tv__ngram_range': (1, 2),\n",
       "   'tv__stop_words': 'english'},\n",
       "  'y_test_hat': array([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         1, 0])},\n",
       " 6: {'vectorizer': 'tv',\n",
       "  'model': 'knn',\n",
       "  'features': [],\n",
       "  'train_score': 0.5506666666666666,\n",
       "  'test_score': 0.5133333333333333,\n",
       "  'best_params': {'knn__n_neighbors': 45,\n",
       "   'tv__max_df': 1.0,\n",
       "   'tv__max_features': 5000,\n",
       "   'tv__min_df': 1,\n",
       "   'tv__ngram_range': (1, 1),\n",
       "   'tv__stop_words': None},\n",
       "  'y_test_hat': array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])},\n",
       " 7: {'vectorizer': 'tv',\n",
       "  'model': 'rf',\n",
       "  'features': [],\n",
       "  'train_score': 0.6813333333333333,\n",
       "  'test_score': 0.6546666666666666,\n",
       "  'best_params': {'rf__n_estimators': 8,\n",
       "   'tv__max_df': 0.95,\n",
       "   'tv__max_features': 8000,\n",
       "   'tv__min_df': 2,\n",
       "   'tv__ngram_range': (1, 1),\n",
       "   'tv__stop_words': 'english'},\n",
       "  'y_test_hat': array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1])},\n",
       " 8: {'vectorizer': 'tv',\n",
       "  'model': 'ab',\n",
       "  'features': [],\n",
       "  'train_score': 0.6564444444444444,\n",
       "  'test_score': 0.648,\n",
       "  'best_params': {'ab__n_estimators': 125,\n",
       "   'tv__max_df': 0.95,\n",
       "   'tv__max_features': 5000,\n",
       "   'tv__min_df': 2,\n",
       "   'tv__ngram_range': (1, 2),\n",
       "   'tv__stop_words': 'english'},\n",
       "  'y_test_hat': array([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Models out of Pipeline Function \n",
    "\n",
    "plt.bar(range(len(model_scores_1)), list(model_scores_1.values()), align='center')\n",
    "plt.xticks(range(len(model_scores_1)), list(model_scores_1.keys()), rotation = 'vertical');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNomial w/ CV Train</th>\n",
       "      <td>0.799556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNomial w/ CV Test</th>\n",
       "      <td>0.590667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg and CV Train</th>\n",
       "      <td>0.940889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg and CV Test</th>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree and TFDIF Train</th>\n",
       "      <td>0.799556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree and TFDIF Test</th>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "MultiNomial w/ CV Train       0.799556\n",
       "MultiNomial w/ CV Test        0.590667\n",
       "LogReg and CV Train           0.940889\n",
       "LogReg and CV Test            0.696000\n",
       "DecisionTree and TFDIF Train  0.799556\n",
       "DecisionTree and TFDIF Test   0.556000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict_1 = pd.DataFrame.from_dict(model_scores_1, orient='index')\n",
    "model_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "      <th>y_test_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>0.682667</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.514222</td>\n",
       "      <td>0.504</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.666222</td>\n",
       "      <td>0.690667</td>\n",
       "      <td>{'cv__max_df': 0.95, 'cv__max_features': 8000,...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.644</td>\n",
       "      <td>{'ab__n_estimators': 125, 'cv__max_df': 1.0, '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>{'svc__kernel': 'linear', 'tv__max_df': 1.0, '...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.550667</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>{'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>0.654667</td>\n",
       "      <td>{'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.656444</td>\n",
       "      <td>0.648</td>\n",
       "      <td>{'ab__n_estimators': 125, 'tv__max_df': 0.95, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vectorizer model features train_score test_score  \\\n",
       "1         cv   svc       []    0.663556   0.682667   \n",
       "2         cv   knn       []    0.514222      0.504   \n",
       "3         cv    rf       []    0.666222   0.690667   \n",
       "4         cv    ab       []    0.665333      0.644   \n",
       "5         tv   svc       []    0.701333   0.714667   \n",
       "6         tv   knn       []    0.550667   0.513333   \n",
       "7         tv    rf       []    0.681333   0.654667   \n",
       "8         tv    ab       []    0.656444      0.648   \n",
       "\n",
       "                                         best_params  \\\n",
       "1  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "2  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "3  {'cv__max_df': 0.95, 'cv__max_features': 8000,...   \n",
       "4  {'ab__n_estimators': 125, 'cv__max_df': 1.0, '...   \n",
       "5  {'svc__kernel': 'linear', 'tv__max_df': 1.0, '...   \n",
       "6  {'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...   \n",
       "7  {'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...   \n",
       "8  {'ab__n_estimators': 125, 'tv__max_df': 0.95, ...   \n",
       "\n",
       "                                          y_test_hat  \n",
       "1  [1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...  \n",
       "2  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...  \n",
       "4  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...  \n",
       "5  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...  \n",
       "7  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...  \n",
       "8  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Models in Pipeline Function (first time ran)\n",
    "\n",
    "df_scores = pd.DataFrame(model_scores)\n",
    "df_scores.sort_values(ascending=False, by='test_score',axis=1)\n",
    "\n",
    "df_scores = df_scores.T\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "      <th>y_test_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>lr</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.692889</td>\n",
       "      <td>0.696</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 8000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>bnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>0.670667</td>\n",
       "      <td>{'bnb__alpha': 1.0, 'cv__max_df': 1.0, 'cv__ma...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv</td>\n",
       "      <td>mnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.684</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 8000, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv</td>\n",
       "      <td>lr</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.694222</td>\n",
       "      <td>0.692</td>\n",
       "      <td>{'lr__C': 1, 'lr__penalty': 'l2', 'tv__max_df'...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>bnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.672</td>\n",
       "      <td>{'bnb__alpha': 1.0, 'tv__max_df': 1.0, 'tv__ma...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tv</td>\n",
       "      <td>mnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>{'mnb__alpha': 1.2, 'tv__max_df': 1.0, 'tv__ma...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vectorizer model features train_score test_score  \\\n",
       "1         cv    lr       []    0.692889      0.696   \n",
       "2         cv   bnb       []    0.663556   0.670667   \n",
       "3         cv   mnb       []        0.68      0.684   \n",
       "4         tv    lr       []    0.694222      0.692   \n",
       "5         tv   bnb       []    0.667111      0.672   \n",
       "6         tv   mnb       []    0.676444   0.681333   \n",
       "\n",
       "                                         best_params  \\\n",
       "1  {'cv__max_df': 1.0, 'cv__max_features': 8000, ...   \n",
       "2  {'bnb__alpha': 1.0, 'cv__max_df': 1.0, 'cv__ma...   \n",
       "3  {'cv__max_df': 1.0, 'cv__max_features': 8000, ...   \n",
       "4  {'lr__C': 1, 'lr__penalty': 'l2', 'tv__max_df'...   \n",
       "5  {'bnb__alpha': 1.0, 'tv__max_df': 1.0, 'tv__ma...   \n",
       "6  {'mnb__alpha': 1.2, 'tv__max_df': 1.0, 'tv__ma...   \n",
       "\n",
       "                                          y_test_hat  \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...  \n",
       "2  [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...  \n",
       "3  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...  \n",
       "4  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...  \n",
       "5  [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...  \n",
       "6  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Models in pipeline (2nd time ran)\n",
    "\n",
    "df_scores_2 = pd.DataFrame(model_scores_2)\n",
    "df_scores_2.sort_values(ascending=False, by='test_score',axis=1)\n",
    "\n",
    "df_scores_2 = df_scores_2.T\n",
    "df_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "      <th>y_test_hat</th>\n",
       "      <th>model_vect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>0.682667</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>svc cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.514222</td>\n",
       "      <td>0.504</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>knn cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.666222</td>\n",
       "      <td>0.690667</td>\n",
       "      <td>{'cv__max_df': 0.95, 'cv__max_features': 8000,...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>rf cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.644</td>\n",
       "      <td>{'ab__n_estimators': 125, 'cv__max_df': 1.0, '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>ab cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>{'svc__kernel': 'linear', 'tv__max_df': 1.0, '...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "      <td>svc tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.550667</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>{'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>knn tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>0.654667</td>\n",
       "      <td>{'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>rf tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.656444</td>\n",
       "      <td>0.648</td>\n",
       "      <td>{'ab__n_estimators': 125, 'tv__max_df': 0.95, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>ab tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>lr</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.692889</td>\n",
       "      <td>0.696</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 8000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>bnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>0.670667</td>\n",
       "      <td>{'bnb__alpha': 1.0, 'cv__max_df': 1.0, 'cv__ma...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv</td>\n",
       "      <td>mnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.684</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 8000, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tv</td>\n",
       "      <td>lr</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.694222</td>\n",
       "      <td>0.692</td>\n",
       "      <td>{'lr__C': 1, 'lr__penalty': 'l2', 'tv__max_df'...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>bnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>0.672</td>\n",
       "      <td>{'bnb__alpha': 1.0, 'tv__max_df': 1.0, 'tv__ma...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tv</td>\n",
       "      <td>mnb</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.676444</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>{'mnb__alpha': 1.2, 'tv__max_df': 1.0, 'tv__ma...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vectorizer model features train_score test_score  \\\n",
       "1         cv   svc       []    0.663556   0.682667   \n",
       "2         cv   knn       []    0.514222      0.504   \n",
       "3         cv    rf       []    0.666222   0.690667   \n",
       "4         cv    ab       []    0.665333      0.644   \n",
       "5         tv   svc       []    0.701333   0.714667   \n",
       "6         tv   knn       []    0.550667   0.513333   \n",
       "7         tv    rf       []    0.681333   0.654667   \n",
       "8         tv    ab       []    0.656444      0.648   \n",
       "1         cv    lr       []    0.692889      0.696   \n",
       "2         cv   bnb       []    0.663556   0.670667   \n",
       "3         cv   mnb       []        0.68      0.684   \n",
       "4         tv    lr       []    0.694222      0.692   \n",
       "5         tv   bnb       []    0.667111      0.672   \n",
       "6         tv   mnb       []    0.676444   0.681333   \n",
       "\n",
       "                                         best_params  \\\n",
       "1  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "2  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "3  {'cv__max_df': 0.95, 'cv__max_features': 8000,...   \n",
       "4  {'ab__n_estimators': 125, 'cv__max_df': 1.0, '...   \n",
       "5  {'svc__kernel': 'linear', 'tv__max_df': 1.0, '...   \n",
       "6  {'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...   \n",
       "7  {'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...   \n",
       "8  {'ab__n_estimators': 125, 'tv__max_df': 0.95, ...   \n",
       "1  {'cv__max_df': 1.0, 'cv__max_features': 8000, ...   \n",
       "2  {'bnb__alpha': 1.0, 'cv__max_df': 1.0, 'cv__ma...   \n",
       "3  {'cv__max_df': 1.0, 'cv__max_features': 8000, ...   \n",
       "4  {'lr__C': 1, 'lr__penalty': 'l2', 'tv__max_df'...   \n",
       "5  {'bnb__alpha': 1.0, 'tv__max_df': 1.0, 'tv__ma...   \n",
       "6  {'mnb__alpha': 1.2, 'tv__max_df': 1.0, 'tv__ma...   \n",
       "\n",
       "                                          y_test_hat model_vect  \n",
       "1  [1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...     svc cv  \n",
       "2  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     knn cv  \n",
       "3  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...      rf cv  \n",
       "4  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...      ab cv  \n",
       "5  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...     svc tv  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...     knn tv  \n",
       "7  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...      rf tv  \n",
       "8  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...      ab tv  \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...        NaN  \n",
       "2  [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...        NaN  \n",
       "3  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...        NaN  \n",
       "4  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...        NaN  \n",
       "5  [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...        NaN  \n",
       "6  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...        NaN  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models ran in pipeline\n",
    "df_scores_combined = pd.concat([df_scores, df_scores_2])\n",
    "\n",
    "df_scores_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_combined['model_vect'] = df_scores['model'] + ' '  + df_scores['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "      <th>y_test_hat</th>\n",
       "      <th>model_vect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>0.682667</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>svc cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.514222</td>\n",
       "      <td>0.504</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__max_features': 5000, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>knn cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.666222</td>\n",
       "      <td>0.690667</td>\n",
       "      <td>{'cv__max_df': 0.95, 'cv__max_features': 8000,...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>rf cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.644</td>\n",
       "      <td>{'ab__n_estimators': 125, 'cv__max_df': 1.0, '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>ab cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tv</td>\n",
       "      <td>svc</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.701333</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>{'svc__kernel': 'linear', 'tv__max_df': 1.0, '...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "      <td>svc tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tv</td>\n",
       "      <td>knn</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.550667</td>\n",
       "      <td>0.513333</td>\n",
       "      <td>{'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>knn tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tv</td>\n",
       "      <td>rf</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.681333</td>\n",
       "      <td>0.654667</td>\n",
       "      <td>{'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>rf tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tv</td>\n",
       "      <td>ab</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.656444</td>\n",
       "      <td>0.648</td>\n",
       "      <td>{'ab__n_estimators': 125, 'tv__max_df': 0.95, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>ab tv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vectorizer model features train_score test_score  \\\n",
       "1         cv   svc       []    0.663556   0.682667   \n",
       "2         cv   knn       []    0.514222      0.504   \n",
       "3         cv    rf       []    0.666222   0.690667   \n",
       "4         cv    ab       []    0.665333      0.644   \n",
       "5         tv   svc       []    0.701333   0.714667   \n",
       "6         tv   knn       []    0.550667   0.513333   \n",
       "7         tv    rf       []    0.681333   0.654667   \n",
       "8         tv    ab       []    0.656444      0.648   \n",
       "\n",
       "                                         best_params  \\\n",
       "1  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "2  {'cv__max_df': 1.0, 'cv__max_features': 5000, ...   \n",
       "3  {'cv__max_df': 0.95, 'cv__max_features': 8000,...   \n",
       "4  {'ab__n_estimators': 125, 'cv__max_df': 1.0, '...   \n",
       "5  {'svc__kernel': 'linear', 'tv__max_df': 1.0, '...   \n",
       "6  {'knn__n_neighbors': 45, 'tv__max_df': 1.0, 't...   \n",
       "7  {'rf__n_estimators': 8, 'tv__max_df': 0.95, 't...   \n",
       "8  {'ab__n_estimators': 125, 'tv__max_df': 0.95, ...   \n",
       "\n",
       "                                          y_test_hat model_vect  \n",
       "1  [1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, ...     svc cv  \n",
       "2  [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     knn cv  \n",
       "3  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...      rf cv  \n",
       "4  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, ...      ab cv  \n",
       "5  [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...     svc tv  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...     knn tv  \n",
       "7  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...      rf tv  \n",
       "8  [0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, ...      ab tv  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test Scores')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "\n",
    "sns.barplot(ax=axes[0],x=\"model_vect\", y=\"train_score\", hue=\"vectorizer\", data=df_scores).set_title('Train Scores')\n",
    "sns.barplot(ax=axes[1],x=\"model_vect\", y=\"test_score\", hue=\"vectorizer\", data=df_scores).set_title('Test Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -0.26894\n",
      "Feature: 1, Score: -0.17824\n",
      "Feature: 2, Score: 0.18000\n",
      "Feature: 3, Score: -0.24269\n",
      "Feature: 4, Score: -0.19038\n",
      "Feature: 5, Score: -0.34936\n",
      "Feature: 6, Score: -0.34936\n",
      "Feature: 7, Score: 0.32593\n",
      "Feature: 8, Score: 0.32593\n",
      "Feature: 9, Score: 0.18000\n",
      "Feature: 10, Score: 0.18000\n",
      "Feature: 11, Score: -0.24269\n",
      "Feature: 12, Score: 0.10755\n",
      "Feature: 13, Score: -0.34936\n",
      "Feature: 14, Score: -0.24269\n",
      "Feature: 15, Score: 0.16873\n",
      "Feature: 16, Score: 0.20904\n",
      "Feature: 17, Score: -0.15061\n",
      "Feature: 18, Score: 0.20904\n",
      "Feature: 19, Score: 0.16873\n",
      "Feature: 20, Score: 0.20904\n",
      "Feature: 21, Score: 0.13633\n",
      "Feature: 22, Score: 0.28366\n",
      "Feature: 23, Score: -0.13126\n",
      "Feature: 24, Score: 0.32593\n",
      "Feature: 25, Score: -0.24269\n",
      "Feature: 26, Score: -0.26894\n",
      "Feature: 27, Score: -0.26894\n",
      "Feature: 28, Score: -0.15061\n",
      "Feature: 29, Score: -0.34936\n",
      "Feature: 30, Score: -0.17824\n",
      "Feature: 31, Score: -0.17824\n",
      "Feature: 32, Score: 0.20904\n",
      "Feature: 33, Score: 0.18000\n",
      "Feature: 34, Score: -0.15061\n",
      "Feature: 35, Score: -0.34936\n",
      "Feature: 36, Score: -0.24269\n",
      "Feature: 37, Score: -0.30304\n",
      "Feature: 38, Score: 0.22831\n",
      "Feature: 39, Score: -0.26894\n",
      "Feature: 40, Score: 0.20904\n",
      "Feature: 41, Score: 0.18000\n",
      "Feature: 42, Score: 0.32593\n",
      "Feature: 43, Score: -0.30304\n",
      "Feature: 44, Score: -0.34936\n",
      "Feature: 45, Score: 0.20904\n",
      "Feature: 46, Score: 0.38637\n",
      "Feature: 47, Score: 0.38637\n",
      "Feature: 48, Score: -0.34936\n",
      "Feature: 49, Score: -0.26894\n",
      "Feature: 50, Score: -0.41564\n",
      "Feature: 51, Score: -0.34936\n",
      "Feature: 52, Score: -0.30304\n",
      "Feature: 53, Score: 0.19323\n",
      "Feature: 54, Score: -0.22178\n",
      "Feature: 55, Score: -0.26894\n",
      "Feature: 56, Score: 0.32593\n",
      "Feature: 57, Score: 0.32593\n",
      "Feature: 58, Score: 0.15900\n",
      "Feature: 59, Score: -0.34936\n",
      "Feature: 60, Score: 0.32593\n",
      "Feature: 61, Score: -0.41564\n",
      "Feature: 62, Score: 0.10755\n",
      "Feature: 63, Score: -0.41564\n",
      "Feature: 64, Score: 0.14301\n",
      "Feature: 65, Score: -0.20467\n",
      "Feature: 66, Score: -0.24269\n",
      "Feature: 67, Score: -0.41564\n",
      "Feature: 68, Score: 0.18000\n",
      "Feature: 69, Score: -0.34936\n",
      "Feature: 70, Score: -0.20467\n",
      "Feature: 71, Score: -0.13126\n",
      "Feature: 72, Score: -0.20467\n",
      "Feature: 73, Score: -0.13126\n",
      "Feature: 74, Score: -0.41564\n",
      "Feature: 75, Score: 0.32593\n",
      "Feature: 76, Score: -0.24269\n",
      "Feature: 77, Score: -0.24269\n",
      "Feature: 78, Score: 0.14301\n",
      "Feature: 79, Score: -0.20467\n",
      "Feature: 80, Score: 0.25243\n",
      "Feature: 81, Score: 0.19323\n",
      "Feature: 82, Score: -0.34936\n",
      "Feature: 83, Score: 0.20904\n",
      "Feature: 84, Score: 0.20904\n",
      "Feature: 85, Score: -0.34936\n",
      "Feature: 86, Score: 0.32593\n",
      "Feature: 87, Score: 0.32593\n",
      "Feature: 88, Score: 0.32593\n",
      "Feature: 89, Score: -0.30304\n",
      "Feature: 90, Score: -0.20467\n",
      "Feature: 91, Score: 0.28366\n",
      "Feature: 92, Score: 0.22831\n",
      "Feature: 93, Score: -0.24269\n",
      "Feature: 94, Score: -0.41564\n",
      "Feature: 95, Score: -0.34936\n",
      "Feature: 96, Score: -0.34936\n",
      "Feature: 97, Score: 0.19323\n",
      "Feature: 98, Score: 0.32593\n",
      "Feature: 99, Score: -0.34936\n",
      "Feature: 100, Score: -0.26894\n",
      "Feature: 101, Score: -0.30304\n",
      "Feature: 102, Score: -0.20467\n",
      "Feature: 103, Score: -0.34936\n",
      "Feature: 104, Score: -0.19038\n",
      "Feature: 105, Score: -0.41564\n",
      "Feature: 106, Score: -0.20467\n",
      "Feature: 107, Score: 0.38637\n",
      "Feature: 108, Score: 0.15900\n",
      "Feature: 109, Score: 0.19323\n",
      "Feature: 110, Score: -0.30304\n",
      "Feature: 111, Score: 0.38637\n",
      "Feature: 112, Score: -0.41564\n",
      "Feature: 113, Score: -0.26894\n",
      "Feature: 114, Score: 0.15900\n",
      "Feature: 115, Score: -0.41564\n",
      "Feature: 116, Score: -0.13126\n",
      "Feature: 117, Score: 0.32593\n",
      "Feature: 118, Score: -0.20467\n",
      "Feature: 119, Score: -0.19038\n",
      "Feature: 120, Score: -0.34936\n",
      "Feature: 121, Score: -0.41564\n",
      "Feature: 122, Score: 0.32593\n",
      "Feature: 123, Score: -0.13126\n",
      "Feature: 124, Score: 0.38637\n",
      "Feature: 125, Score: -0.34936\n",
      "Feature: 126, Score: -0.34936\n",
      "Feature: 127, Score: -0.34936\n",
      "Feature: 128, Score: -0.24269\n",
      "Feature: 129, Score: 0.25243\n",
      "Feature: 130, Score: -0.22178\n",
      "Feature: 131, Score: -0.34936\n",
      "Feature: 132, Score: -0.41564\n",
      "Feature: 133, Score: -0.24269\n",
      "Feature: 134, Score: -0.24269\n",
      "Feature: 135, Score: -0.34936\n",
      "Feature: 136, Score: 0.38637\n",
      "Feature: 137, Score: -0.30304\n",
      "Feature: 138, Score: 0.32593\n",
      "Feature: 139, Score: -0.41564\n",
      "Feature: 140, Score: 0.38637\n",
      "Feature: 141, Score: 0.15900\n",
      "Feature: 142, Score: 0.38637\n",
      "Feature: 143, Score: 0.38637\n",
      "Feature: 144, Score: 0.28366\n",
      "Feature: 145, Score: 0.38637\n",
      "Feature: 146, Score: -0.34936\n",
      "Feature: 147, Score: 0.38637\n",
      "Feature: 148, Score: 0.38637\n",
      "Feature: 149, Score: 0.32593\n",
      "Feature: 150, Score: 0.15900\n",
      "Feature: 151, Score: 0.22831\n",
      "Feature: 152, Score: 0.32593\n",
      "Feature: 153, Score: -0.22178\n",
      "Feature: 154, Score: -0.19038\n",
      "Feature: 155, Score: 0.32593\n",
      "Feature: 156, Score: -0.30304\n",
      "Feature: 157, Score: -0.24269\n",
      "Feature: 158, Score: -0.26894\n",
      "Feature: 159, Score: -0.24269\n",
      "Feature: 160, Score: -0.34936\n",
      "Feature: 161, Score: 0.15900\n",
      "Feature: 162, Score: -0.34936\n",
      "Feature: 163, Score: 0.32593\n",
      "Feature: 164, Score: -0.26894\n",
      "Feature: 165, Score: -0.34936\n",
      "Feature: 166, Score: -0.34936\n",
      "Feature: 167, Score: 0.22831\n",
      "Feature: 168, Score: 0.20904\n",
      "Feature: 169, Score: -0.26894\n",
      "Feature: 170, Score: 0.38637\n",
      "Feature: 171, Score: 0.38637\n",
      "Feature: 172, Score: 0.38637\n",
      "Feature: 173, Score: 0.38637\n",
      "Feature: 174, Score: -0.34936\n",
      "Feature: 175, Score: -0.20467\n",
      "Feature: 176, Score: 0.38637\n",
      "Feature: 177, Score: 0.22831\n",
      "Feature: 178, Score: 0.22831\n",
      "Feature: 179, Score: -0.24269\n",
      "Feature: 180, Score: 0.32593\n",
      "Feature: 181, Score: -0.41564\n",
      "Feature: 182, Score: -0.34936\n",
      "Feature: 183, Score: 0.32593\n",
      "Feature: 184, Score: -0.34936\n",
      "Feature: 185, Score: -0.26894\n",
      "Feature: 186, Score: -0.41564\n",
      "Feature: 187, Score: 0.28366\n",
      "Feature: 188, Score: -0.24269\n",
      "Feature: 189, Score: -0.24269\n",
      "Feature: 190, Score: 0.32593\n",
      "Feature: 191, Score: 0.25243\n",
      "Feature: 192, Score: -0.34936\n",
      "Feature: 193, Score: 0.22831\n",
      "Feature: 194, Score: 0.28366\n",
      "Feature: 195, Score: -0.24269\n",
      "Feature: 196, Score: -0.34936\n",
      "Feature: 197, Score: 0.32593\n",
      "Feature: 198, Score: 0.28366\n",
      "Feature: 199, Score: 0.10755\n",
      "Feature: 200, Score: 0.32593\n",
      "Feature: 201, Score: 0.25243\n",
      "Feature: 202, Score: -0.30304\n",
      "Feature: 203, Score: 0.38637\n",
      "Feature: 204, Score: 0.15051\n",
      "Feature: 205, Score: 0.38637\n",
      "Feature: 206, Score: 0.38637\n",
      "Feature: 207, Score: 0.18000\n",
      "Feature: 208, Score: -0.41564\n",
      "Feature: 209, Score: -0.19038\n",
      "Feature: 210, Score: 0.09769\n",
      "Feature: 211, Score: 0.14301\n",
      "Feature: 212, Score: -0.15061\n",
      "Feature: 213, Score: -0.20467\n",
      "Feature: 214, Score: -0.41564\n",
      "Feature: 215, Score: -0.26894\n",
      "Feature: 216, Score: 0.18000\n",
      "Feature: 217, Score: -0.20467\n",
      "Feature: 218, Score: 0.28366\n",
      "Feature: 219, Score: -0.34936\n",
      "Feature: 220, Score: 0.32593\n",
      "Feature: 221, Score: 0.32593\n",
      "Feature: 222, Score: 0.38637\n",
      "Feature: 223, Score: -0.26894\n",
      "Feature: 224, Score: -0.24269\n",
      "Feature: 225, Score: -0.19038\n",
      "Feature: 226, Score: 0.25243\n",
      "Feature: 227, Score: -0.24269\n",
      "Feature: 228, Score: -0.41564\n",
      "Feature: 229, Score: 0.32593\n",
      "Feature: 230, Score: -0.41564\n",
      "Feature: 231, Score: -0.34936\n",
      "Feature: 232, Score: -0.24269\n",
      "Feature: 233, Score: 0.20904\n",
      "Feature: 234, Score: 0.13633\n",
      "Feature: 235, Score: -0.19038\n",
      "Feature: 236, Score: -0.38076\n",
      "Feature: 237, Score: -0.15865\n",
      "Feature: 238, Score: -0.22178\n",
      "Feature: 239, Score: -0.26894\n",
      "Feature: 240, Score: -0.34936\n",
      "Feature: 241, Score: 0.16873\n",
      "Feature: 242, Score: 0.25243\n",
      "Feature: 243, Score: -0.19038\n",
      "Feature: 244, Score: 0.18000\n",
      "Feature: 245, Score: -0.41564\n",
      "Feature: 246, Score: -0.38076\n",
      "Feature: 247, Score: -0.24269\n",
      "Feature: 248, Score: -0.34936\n",
      "Feature: 249, Score: -0.34936\n",
      "Feature: 250, Score: 0.38637\n",
      "Feature: 251, Score: 0.25243\n",
      "Feature: 252, Score: -0.41564\n",
      "Feature: 253, Score: -0.20467\n",
      "Feature: 254, Score: -0.19038\n",
      "Feature: 255, Score: -0.30304\n",
      "Feature: 256, Score: -0.41564\n",
      "Feature: 257, Score: -0.26894\n",
      "Feature: 258, Score: -0.19038\n",
      "Feature: 259, Score: 0.32593\n",
      "Feature: 260, Score: -0.30304\n",
      "Feature: 261, Score: -0.24269\n",
      "Feature: 262, Score: -0.22178\n",
      "Feature: 263, Score: -0.41564\n",
      "Feature: 264, Score: 0.18000\n",
      "Feature: 265, Score: 0.19323\n",
      "Feature: 266, Score: -0.41564\n",
      "Feature: 267, Score: -0.26894\n",
      "Feature: 268, Score: -0.24269\n",
      "Feature: 269, Score: -0.41564\n",
      "Feature: 270, Score: 0.28366\n",
      "Feature: 271, Score: 0.19323\n",
      "Feature: 272, Score: -0.26894\n",
      "Feature: 273, Score: -0.24269\n",
      "Feature: 274, Score: 0.28366\n",
      "Feature: 275, Score: -0.30304\n",
      "Feature: 276, Score: -0.38076\n",
      "Feature: 277, Score: 0.28366\n",
      "Feature: 278, Score: 0.28366\n",
      "Feature: 279, Score: -0.34936\n",
      "Feature: 280, Score: -0.34936\n",
      "Feature: 281, Score: -0.41564\n",
      "Feature: 282, Score: -0.41564\n",
      "Feature: 283, Score: 0.32593\n",
      "Feature: 284, Score: 0.16873\n",
      "Feature: 285, Score: -0.30304\n",
      "Feature: 286, Score: 0.25243\n",
      "Feature: 287, Score: 0.22831\n",
      "Feature: 288, Score: 0.38637\n",
      "Feature: 289, Score: -0.34936\n",
      "Feature: 290, Score: 0.25243\n",
      "Feature: 291, Score: 0.22831\n",
      "Feature: 292, Score: -0.24269\n",
      "Feature: 293, Score: 0.20904\n",
      "Feature: 294, Score: 0.32593\n",
      "Feature: 295, Score: 0.38637\n",
      "Feature: 296, Score: 0.09769\n",
      "Feature: 297, Score: -0.40934\n",
      "Feature: 298, Score: -0.19038\n",
      "Feature: 299, Score: 0.32593\n",
      "Feature: 300, Score: 0.22831\n",
      "Feature: 301, Score: -0.41564\n",
      "Feature: 302, Score: -0.19038\n",
      "Feature: 303, Score: -0.24269\n",
      "Feature: 304, Score: -0.30304\n",
      "Feature: 305, Score: -0.24269\n",
      "Feature: 306, Score: -0.30304\n",
      "Feature: 307, Score: 0.32593\n",
      "Feature: 308, Score: -0.41564\n",
      "Feature: 309, Score: -0.30304\n",
      "Feature: 310, Score: -0.34936\n",
      "Feature: 311, Score: -0.41564\n",
      "Feature: 312, Score: 0.28366\n",
      "Feature: 313, Score: -0.30304\n",
      "Feature: 314, Score: -0.26894\n",
      "Feature: 315, Score: -0.41564\n",
      "Feature: 316, Score: -0.17824\n",
      "Feature: 317, Score: -0.24269\n",
      "Feature: 318, Score: 0.20904\n",
      "Feature: 319, Score: 0.18000\n",
      "Feature: 320, Score: 0.38637\n",
      "Feature: 321, Score: -0.13126\n",
      "Feature: 322, Score: -0.13126\n",
      "Feature: 323, Score: 0.15900\n",
      "Feature: 324, Score: 0.18000\n",
      "Feature: 325, Score: 0.32593\n",
      "Feature: 326, Score: -0.26894\n",
      "Feature: 327, Score: 0.32593\n",
      "Feature: 328, Score: -0.30304\n",
      "Feature: 329, Score: 0.38637\n",
      "Feature: 330, Score: -0.26894\n",
      "Feature: 331, Score: -0.24269\n",
      "Feature: 332, Score: 0.32593\n",
      "Feature: 333, Score: -0.41564\n",
      "Feature: 334, Score: -0.19038\n",
      "Feature: 335, Score: 0.32593\n",
      "Feature: 336, Score: 0.38637\n",
      "Feature: 337, Score: -0.26894\n",
      "Feature: 338, Score: -0.30304\n",
      "Feature: 339, Score: -0.30304\n",
      "Feature: 340, Score: 0.20904\n",
      "Feature: 341, Score: -0.26894\n",
      "Feature: 342, Score: 0.28366\n",
      "Feature: 343, Score: 0.38637\n",
      "Feature: 344, Score: 0.28366\n",
      "Feature: 345, Score: -0.41564\n",
      "Feature: 346, Score: -0.24269\n",
      "Feature: 347, Score: -0.26894\n",
      "Feature: 348, Score: -0.34936\n",
      "Feature: 349, Score: -0.19038\n",
      "Feature: 350, Score: 0.38637\n",
      "Feature: 351, Score: 0.38637\n",
      "Feature: 352, Score: 0.15051\n",
      "Feature: 353, Score: 0.38637\n",
      "Feature: 354, Score: 0.38637\n",
      "Feature: 355, Score: 0.38637\n",
      "Feature: 356, Score: 0.25243\n",
      "Feature: 357, Score: 0.25243\n",
      "Feature: 358, Score: 0.25243\n",
      "Feature: 359, Score: 0.14301\n",
      "Feature: 360, Score: 0.38637\n",
      "Feature: 361, Score: -0.24269\n",
      "Feature: 362, Score: -0.30304\n",
      "Feature: 363, Score: -0.41564\n",
      "Feature: 364, Score: 0.28366\n",
      "Feature: 365, Score: 0.32593\n",
      "Feature: 366, Score: 0.25243\n",
      "Feature: 367, Score: 0.32593\n",
      "Feature: 368, Score: -0.34936\n",
      "Feature: 369, Score: -0.41564\n",
      "Feature: 370, Score: 0.15051\n",
      "Feature: 371, Score: 0.38637\n",
      "Feature: 372, Score: 0.32593\n",
      "Feature: 373, Score: -0.34936\n",
      "Feature: 374, Score: 0.15051\n",
      "Feature: 375, Score: -0.41564\n",
      "Feature: 376, Score: 0.14301\n",
      "Feature: 377, Score: -0.22178\n",
      "Feature: 378, Score: -0.20467\n",
      "Feature: 379, Score: -0.41564\n",
      "Feature: 380, Score: 0.32593\n",
      "Feature: 381, Score: -0.34936\n",
      "Feature: 382, Score: -0.38076\n",
      "Feature: 383, Score: -0.41564\n",
      "Feature: 384, Score: -0.30304\n",
      "Feature: 385, Score: -0.41564\n",
      "Feature: 386, Score: -0.34936\n",
      "Feature: 387, Score: -0.15865\n",
      "Feature: 388, Score: 0.22831\n",
      "Feature: 389, Score: -0.19038\n",
      "Feature: 390, Score: 0.22831\n",
      "Feature: 391, Score: 0.18000\n",
      "Feature: 392, Score: -0.34936\n",
      "Feature: 393, Score: 0.25243\n",
      "Feature: 394, Score: -0.30304\n",
      "Feature: 395, Score: -0.41564\n",
      "Feature: 396, Score: 0.38637\n",
      "Feature: 397, Score: 0.22831\n",
      "Feature: 398, Score: -0.34936\n",
      "Feature: 399, Score: 0.38637\n",
      "Feature: 400, Score: -0.34936\n",
      "Feature: 401, Score: -0.41115\n",
      "Feature: 402, Score: -0.22178\n",
      "Feature: 403, Score: -0.34936\n",
      "Feature: 404, Score: 0.28366\n",
      "Feature: 405, Score: -0.34936\n",
      "Feature: 406, Score: -0.30304\n",
      "Feature: 407, Score: -0.26894\n",
      "Feature: 408, Score: -0.24269\n",
      "Feature: 409, Score: 0.38637\n",
      "Feature: 410, Score: 0.20904\n",
      "Feature: 411, Score: 0.32593\n",
      "Feature: 412, Score: -0.26894\n",
      "Feature: 413, Score: -0.34936\n",
      "Feature: 414, Score: -0.41564\n",
      "Feature: 415, Score: -0.30304\n",
      "Feature: 416, Score: 0.38637\n",
      "Feature: 417, Score: 0.38637\n",
      "Feature: 418, Score: 0.28366\n",
      "Feature: 419, Score: -0.34936\n",
      "Feature: 420, Score: 0.22831\n",
      "Feature: 421, Score: -0.26894\n",
      "Feature: 422, Score: 0.38637\n",
      "Feature: 423, Score: 0.28366\n",
      "Feature: 424, Score: -0.34936\n",
      "Feature: 425, Score: -0.26894\n",
      "Feature: 426, Score: 0.38637\n",
      "Feature: 427, Score: -0.30304\n",
      "Feature: 428, Score: 0.19323\n",
      "Feature: 429, Score: 0.32593\n",
      "Feature: 430, Score: -0.38076\n",
      "Feature: 431, Score: -0.26252\n",
      "Feature: 432, Score: -0.30304\n",
      "Feature: 433, Score: -0.41564\n",
      "Feature: 434, Score: 0.32593\n",
      "Feature: 435, Score: 0.15900\n",
      "Feature: 436, Score: -0.41564\n",
      "Feature: 437, Score: -0.13126\n",
      "Feature: 438, Score: -0.26894\n",
      "Feature: 439, Score: -0.26894\n",
      "Feature: 440, Score: -0.26894\n",
      "Feature: 441, Score: 0.16873\n",
      "Feature: 442, Score: 0.32593\n",
      "Feature: 443, Score: 0.15900\n",
      "Feature: 444, Score: 0.20904\n",
      "Feature: 445, Score: -0.30304\n",
      "Feature: 446, Score: -0.19038\n",
      "Feature: 447, Score: 0.15900\n",
      "Feature: 448, Score: -0.30304\n",
      "Feature: 449, Score: 0.38637\n",
      "Feature: 450, Score: -0.19038\n",
      "Feature: 451, Score: -0.34936\n",
      "Feature: 452, Score: -0.34936\n",
      "Feature: 453, Score: 0.20904\n",
      "Feature: 454, Score: -0.34936\n",
      "Feature: 455, Score: -0.34936\n",
      "Feature: 456, Score: 0.28366\n",
      "Feature: 457, Score: -0.19038\n",
      "Feature: 458, Score: -0.34936\n",
      "Feature: 459, Score: -0.20467\n",
      "Feature: 460, Score: 0.38637\n",
      "Feature: 461, Score: -0.41564\n",
      "Feature: 462, Score: 0.14301\n",
      "Feature: 463, Score: -0.20467\n",
      "Feature: 464, Score: -0.34936\n",
      "Feature: 465, Score: -0.26894\n",
      "Feature: 466, Score: 0.32593\n",
      "Feature: 467, Score: -0.22178\n",
      "Feature: 468, Score: -0.24269\n",
      "Feature: 469, Score: -0.20467\n",
      "Feature: 470, Score: -0.30304\n",
      "Feature: 471, Score: -0.30304\n",
      "Feature: 472, Score: -0.30304\n",
      "Feature: 473, Score: 0.18000\n",
      "Feature: 474, Score: 0.25243\n",
      "Feature: 475, Score: 0.25243\n",
      "Feature: 476, Score: -0.15061\n",
      "Feature: 477, Score: 0.32593\n",
      "Feature: 478, Score: 0.38637\n",
      "Feature: 479, Score: -0.34936\n",
      "Feature: 480, Score: -0.34936\n",
      "Feature: 481, Score: 0.38637\n",
      "Feature: 482, Score: -0.41564\n",
      "Feature: 483, Score: 0.32593\n",
      "Feature: 484, Score: 0.14301\n",
      "Feature: 485, Score: -0.34936\n",
      "Feature: 486, Score: 0.22831\n",
      "Feature: 487, Score: -0.26894\n",
      "Feature: 488, Score: 0.28366\n",
      "Feature: 489, Score: -0.40934\n",
      "Feature: 490, Score: 0.18000\n",
      "Feature: 491, Score: 0.38637\n",
      "Feature: 492, Score: -0.15061\n",
      "Feature: 493, Score: -0.34936\n",
      "Feature: 494, Score: -0.24269\n",
      "Feature: 495, Score: 0.20904\n",
      "Feature: 496, Score: 0.32593\n",
      "Feature: 497, Score: -0.26894\n",
      "Feature: 498, Score: -0.41564\n",
      "Feature: 499, Score: -0.41564\n",
      "Feature: 500, Score: 0.38637\n",
      "Feature: 501, Score: -0.41564\n",
      "Feature: 502, Score: 0.50486\n",
      "Feature: 503, Score: 0.28366\n",
      "Feature: 504, Score: -0.34936\n",
      "Feature: 505, Score: -0.41564\n",
      "Feature: 506, Score: 0.28366\n",
      "Feature: 507, Score: 0.38637\n",
      "Feature: 508, Score: 0.22831\n",
      "Feature: 509, Score: -0.19038\n",
      "Feature: 510, Score: 0.38637\n",
      "Feature: 511, Score: -0.15061\n",
      "Feature: 512, Score: 0.28366\n",
      "Feature: 513, Score: 0.32593\n",
      "Feature: 514, Score: -0.27410\n",
      "Feature: 515, Score: -0.34936\n",
      "Feature: 516, Score: 0.38637\n",
      "Feature: 517, Score: 0.16873\n",
      "Feature: 518, Score: -0.26894\n",
      "Feature: 519, Score: 0.38637\n",
      "Feature: 520, Score: 0.28366\n",
      "Feature: 521, Score: -0.24269\n",
      "Feature: 522, Score: -0.30304\n",
      "Feature: 523, Score: -0.22178\n",
      "Feature: 524, Score: -0.22178\n",
      "Feature: 525, Score: 0.32593\n",
      "Feature: 526, Score: -0.26894\n",
      "Feature: 527, Score: -0.41564\n",
      "Feature: 528, Score: -0.24269\n",
      "Feature: 529, Score: -0.41564\n",
      "Feature: 530, Score: 0.38637\n",
      "Feature: 531, Score: 0.28366\n",
      "Feature: 532, Score: 0.25243\n",
      "Feature: 533, Score: -0.41564\n",
      "Feature: 534, Score: 0.20904\n",
      "Feature: 535, Score: -0.30304\n",
      "Feature: 536, Score: -0.22178\n",
      "Feature: 537, Score: 0.15051\n",
      "Feature: 538, Score: -0.34936\n",
      "Feature: 539, Score: 0.45662\n",
      "Feature: 540, Score: -0.24269\n",
      "Feature: 541, Score: 0.32593\n",
      "Feature: 542, Score: -0.30304\n",
      "Feature: 543, Score: 0.32593\n",
      "Feature: 544, Score: -0.24269\n",
      "Feature: 545, Score: 0.09769\n",
      "Feature: 546, Score: 0.38637\n",
      "Feature: 547, Score: 0.18000\n",
      "Feature: 548, Score: -0.26894\n",
      "Feature: 549, Score: 0.28366\n",
      "Feature: 550, Score: 0.32593\n",
      "Feature: 551, Score: 0.32593\n",
      "Feature: 552, Score: 0.18000\n",
      "Feature: 553, Score: -0.30304\n",
      "Feature: 554, Score: 0.22831\n",
      "Feature: 555, Score: 0.25243\n",
      "Feature: 556, Score: 0.28366\n",
      "Feature: 557, Score: -0.26894\n",
      "Feature: 558, Score: -0.26894\n",
      "Feature: 559, Score: 0.38637\n",
      "Feature: 560, Score: -0.30304\n",
      "Feature: 561, Score: -0.26894\n",
      "Feature: 562, Score: -0.34936\n",
      "Feature: 563, Score: 0.22831\n",
      "Feature: 564, Score: 0.20904\n",
      "Feature: 565, Score: -0.34936\n",
      "Feature: 566, Score: 0.32593\n",
      "Feature: 567, Score: 0.38637\n",
      "Feature: 568, Score: 0.38637\n",
      "Feature: 569, Score: 0.16873\n",
      "Feature: 570, Score: -0.34936\n",
      "Feature: 571, Score: 0.20904\n",
      "Feature: 572, Score: -0.41564\n",
      "Feature: 573, Score: 0.22831\n",
      "Feature: 574, Score: 0.28366\n",
      "Feature: 575, Score: -0.34936\n",
      "Feature: 576, Score: -0.30304\n",
      "Feature: 577, Score: -0.24269\n",
      "Feature: 578, Score: 0.22831\n",
      "Feature: 579, Score: -0.20467\n",
      "Feature: 580, Score: -0.41564\n",
      "Feature: 581, Score: 0.38637\n",
      "Feature: 582, Score: -0.19038\n",
      "Feature: 583, Score: -0.34936\n",
      "Feature: 584, Score: -0.26894\n",
      "Feature: 585, Score: -0.19038\n",
      "Feature: 586, Score: 0.22831\n",
      "Feature: 587, Score: 0.22831\n",
      "Feature: 588, Score: -0.26894\n",
      "Feature: 589, Score: -0.26894\n",
      "Feature: 590, Score: -0.41564\n",
      "Feature: 591, Score: -0.20467\n",
      "Feature: 592, Score: -0.41564\n",
      "Feature: 593, Score: 0.38637\n",
      "Feature: 594, Score: 0.45662\n",
      "Feature: 595, Score: -0.41564\n",
      "Feature: 596, Score: 0.38637\n",
      "Feature: 597, Score: -0.26894\n",
      "Feature: 598, Score: -0.20467\n",
      "Feature: 599, Score: 0.15900\n",
      "Feature: 600, Score: 0.18000\n",
      "Feature: 601, Score: -0.34936\n",
      "Feature: 602, Score: -0.34936\n",
      "Feature: 603, Score: -0.19038\n",
      "Feature: 604, Score: 0.28366\n",
      "Feature: 605, Score: -0.20467\n",
      "Feature: 606, Score: -0.24269\n",
      "Feature: 607, Score: 0.38637\n",
      "Feature: 608, Score: -0.34936\n",
      "Feature: 609, Score: 0.14301\n",
      "Feature: 610, Score: -0.34936\n",
      "Feature: 611, Score: 0.18000\n",
      "Feature: 612, Score: 0.16873\n",
      "Feature: 613, Score: -0.34936\n",
      "Feature: 614, Score: -0.17824\n",
      "Feature: 615, Score: 0.32593\n",
      "Feature: 616, Score: -0.15865\n",
      "Feature: 617, Score: 0.28366\n",
      "Feature: 618, Score: 0.22831\n",
      "Feature: 619, Score: -0.41564\n",
      "Feature: 620, Score: -0.24269\n",
      "Feature: 621, Score: -0.20467\n",
      "Feature: 622, Score: 0.28366\n",
      "Feature: 623, Score: -0.41564\n",
      "Feature: 624, Score: 0.38637\n",
      "Feature: 625, Score: -0.41564\n",
      "Feature: 626, Score: 0.22831\n",
      "Feature: 627, Score: 0.38637\n",
      "Feature: 628, Score: -0.41564\n",
      "Feature: 629, Score: -0.41564\n",
      "Feature: 630, Score: 0.32593\n",
      "Feature: 631, Score: 0.15900\n",
      "Feature: 632, Score: -0.34936\n",
      "Feature: 633, Score: -0.31731\n",
      "Feature: 634, Score: -0.20467\n",
      "Feature: 635, Score: -0.24269\n",
      "Feature: 636, Score: -0.24269\n",
      "Feature: 637, Score: -0.41564\n",
      "Feature: 638, Score: -0.41564\n",
      "Feature: 639, Score: -0.26894\n",
      "Feature: 640, Score: -0.15061\n",
      "Feature: 641, Score: -0.15865\n",
      "Feature: 642, Score: -0.24269\n",
      "Feature: 643, Score: 0.19323\n",
      "Feature: 644, Score: -0.12601\n",
      "Feature: 645, Score: 0.38637\n",
      "Feature: 646, Score: -0.34936\n",
      "Feature: 647, Score: -0.30304\n",
      "Feature: 648, Score: -0.41564\n",
      "Feature: 649, Score: -0.26894\n",
      "Feature: 650, Score: 0.38637\n",
      "Feature: 651, Score: -0.34936\n",
      "Feature: 652, Score: -0.41564\n",
      "Feature: 653, Score: -0.24269\n",
      "Feature: 654, Score: 0.32593\n",
      "Feature: 655, Score: 0.22831\n",
      "Feature: 656, Score: -0.34936\n",
      "Feature: 657, Score: -0.30304\n",
      "Feature: 658, Score: 0.28366\n",
      "Feature: 659, Score: -0.34936\n",
      "Feature: 660, Score: 0.38637\n",
      "Feature: 661, Score: -0.41564\n",
      "Feature: 662, Score: -0.34936\n",
      "Feature: 663, Score: -0.34936\n",
      "Feature: 664, Score: -0.24269\n",
      "Feature: 665, Score: -0.26894\n",
      "Feature: 666, Score: -0.34936\n",
      "Feature: 667, Score: -0.20467\n",
      "Feature: 668, Score: 0.38637\n",
      "Feature: 669, Score: 0.38637\n",
      "Feature: 670, Score: -0.41564\n",
      "Feature: 671, Score: 0.38637\n",
      "Feature: 672, Score: 0.38637\n",
      "Feature: 673, Score: 0.22831\n",
      "Feature: 674, Score: -0.41564\n",
      "Feature: 675, Score: 0.38637\n",
      "Feature: 676, Score: -0.19038\n",
      "Feature: 677, Score: -0.34936\n",
      "Feature: 678, Score: 0.38646\n",
      "Feature: 679, Score: 0.38637\n",
      "Feature: 680, Score: -0.30304\n",
      "Feature: 681, Score: 0.28366\n",
      "Feature: 682, Score: 0.16873\n",
      "Feature: 683, Score: -0.13705\n",
      "Feature: 684, Score: -0.13705\n",
      "Feature: 685, Score: 0.13633\n",
      "Feature: 686, Score: 0.32593\n",
      "Feature: 687, Score: -0.40934\n",
      "Feature: 688, Score: -0.15061\n",
      "Feature: 689, Score: 0.28366\n",
      "Feature: 690, Score: 0.28366\n",
      "Feature: 691, Score: 0.28366\n",
      "Feature: 692, Score: 0.38637\n",
      "Feature: 693, Score: 0.15051\n",
      "Feature: 694, Score: 0.38637\n",
      "Feature: 695, Score: -0.34936\n",
      "Feature: 696, Score: -0.34936\n",
      "Feature: 697, Score: -0.22178\n",
      "Feature: 698, Score: -0.41564\n",
      "Feature: 699, Score: -0.34936\n",
      "Feature: 700, Score: -0.34936\n",
      "Feature: 701, Score: -0.15061\n",
      "Feature: 702, Score: -0.19038\n",
      "Feature: 703, Score: -0.24269\n",
      "Feature: 704, Score: -0.41564\n",
      "Feature: 705, Score: -0.34936\n",
      "Feature: 706, Score: -0.34936\n",
      "Feature: 707, Score: -0.19038\n",
      "Feature: 708, Score: -0.41564\n",
      "Feature: 709, Score: 0.22831\n",
      "Feature: 710, Score: -0.41564\n",
      "Feature: 711, Score: 0.22831\n",
      "Feature: 712, Score: -0.30304\n",
      "Feature: 713, Score: -0.20467\n",
      "Feature: 714, Score: -0.34936\n",
      "Feature: 715, Score: -0.26894\n",
      "Feature: 716, Score: -0.34936\n",
      "Feature: 717, Score: -0.30304\n",
      "Feature: 718, Score: 0.22831\n",
      "Feature: 719, Score: -0.30304\n",
      "Feature: 720, Score: 0.20904\n",
      "Feature: 721, Score: -0.34936\n",
      "Feature: 722, Score: -0.24269\n",
      "Feature: 723, Score: 0.38637\n",
      "Feature: 724, Score: -0.34936\n",
      "Feature: 725, Score: 0.38637\n",
      "Feature: 726, Score: -0.17824\n",
      "Feature: 727, Score: 0.25243\n",
      "Feature: 728, Score: -0.30304\n",
      "Feature: 729, Score: -0.41564\n",
      "Feature: 730, Score: -0.34936\n",
      "Feature: 731, Score: 0.38637\n",
      "Feature: 732, Score: -0.24269\n",
      "Feature: 733, Score: 0.16873\n",
      "Feature: 734, Score: -0.34936\n",
      "Feature: 735, Score: -0.20467\n",
      "Feature: 736, Score: 0.32593\n",
      "Feature: 737, Score: -0.26894\n",
      "Feature: 738, Score: -0.34936\n",
      "Feature: 739, Score: 0.32593\n",
      "Feature: 740, Score: -0.19038\n",
      "Feature: 741, Score: -0.19038\n",
      "Feature: 742, Score: 0.28366\n",
      "Feature: 743, Score: -0.41564\n",
      "Feature: 744, Score: -0.34936\n",
      "Feature: 745, Score: 0.20904\n",
      "Feature: 746, Score: -0.30304\n",
      "Feature: 747, Score: -0.41564\n",
      "Feature: 748, Score: -0.26894\n",
      "Feature: 749, Score: 0.18000\n",
      "Feature: 750, Score: -0.24269\n",
      "Feature: 751, Score: 0.32593\n",
      "Feature: 752, Score: 0.38637\n",
      "Feature: 753, Score: -0.15061\n",
      "Feature: 754, Score: -0.19038\n",
      "Feature: 755, Score: 0.32593\n",
      "Feature: 756, Score: 0.38637\n",
      "Feature: 757, Score: 0.32593\n",
      "Feature: 758, Score: -0.41564\n",
      "Feature: 759, Score: -0.34936\n",
      "Feature: 760, Score: -0.26894\n",
      "Feature: 761, Score: 0.38637\n",
      "Feature: 762, Score: -0.24269\n",
      "Feature: 763, Score: 0.16873\n",
      "Feature: 764, Score: -0.41564\n",
      "Feature: 765, Score: 0.28366\n",
      "Feature: 766, Score: -0.19038\n",
      "Feature: 767, Score: 0.19323\n",
      "Feature: 768, Score: 0.15900\n",
      "Feature: 769, Score: -0.26894\n",
      "Feature: 770, Score: -0.13126\n",
      "Feature: 771, Score: -0.13126\n",
      "Feature: 772, Score: -0.41564\n",
      "Feature: 773, Score: 0.25243\n",
      "Feature: 774, Score: 0.22831\n",
      "Feature: 775, Score: -0.30304\n",
      "Feature: 776, Score: -0.26894\n",
      "Feature: 777, Score: -0.20467\n",
      "Feature: 778, Score: -0.30304\n",
      "Feature: 779, Score: -0.26894\n",
      "Feature: 780, Score: 0.28366\n",
      "Feature: 781, Score: 0.18000\n",
      "Feature: 782, Score: 0.20904\n",
      "Feature: 783, Score: -0.26894\n",
      "Feature: 784, Score: 0.38637\n",
      "Feature: 785, Score: -0.20467\n",
      "Feature: 786, Score: -0.24269\n",
      "Feature: 787, Score: 0.38637\n",
      "Feature: 788, Score: -0.30304\n",
      "Feature: 789, Score: 0.28366\n",
      "Feature: 790, Score: -0.24269\n",
      "Feature: 791, Score: -0.17824\n",
      "Feature: 792, Score: -0.34936\n",
      "Feature: 793, Score: -0.34936\n",
      "Feature: 794, Score: 0.38637\n",
      "Feature: 795, Score: -0.24269\n",
      "Feature: 796, Score: 0.32593\n",
      "Feature: 797, Score: 0.10755\n",
      "Feature: 798, Score: -0.24269\n",
      "Feature: 799, Score: -0.34936\n",
      "Feature: 800, Score: -0.41564\n",
      "Feature: 801, Score: 0.28366\n",
      "Feature: 802, Score: -0.30304\n",
      "Feature: 803, Score: -0.34936\n",
      "Feature: 804, Score: 0.25243\n",
      "Feature: 805, Score: -0.30304\n",
      "Feature: 806, Score: -0.20467\n",
      "Feature: 807, Score: 0.19323\n",
      "Feature: 808, Score: 0.28366\n",
      "Feature: 809, Score: 0.32593\n",
      "Feature: 810, Score: 0.38637\n",
      "Feature: 811, Score: 0.32593\n",
      "Feature: 812, Score: 0.28366\n",
      "Feature: 813, Score: -0.20467\n",
      "Feature: 814, Score: 0.20904\n",
      "Feature: 815, Score: -0.41564\n",
      "Feature: 816, Score: -0.30304\n",
      "Feature: 817, Score: 0.28366\n",
      "Feature: 818, Score: 0.13034\n",
      "Feature: 819, Score: -0.34936\n",
      "Feature: 820, Score: 0.16873\n",
      "Feature: 821, Score: -0.26894\n",
      "Feature: 822, Score: 0.38637\n",
      "Feature: 823, Score: -0.26894\n",
      "Feature: 824, Score: -0.19038\n",
      "Feature: 825, Score: 0.25243\n",
      "Feature: 826, Score: 0.38637\n",
      "Feature: 827, Score: -0.30304\n",
      "Feature: 828, Score: -0.26894\n",
      "Feature: 829, Score: -0.34936\n",
      "Feature: 830, Score: 0.38637\n",
      "Feature: 831, Score: 0.28366\n",
      "Feature: 832, Score: -0.17824\n",
      "Feature: 833, Score: 0.32593\n",
      "Feature: 834, Score: -0.26894\n",
      "Feature: 835, Score: -0.34936\n",
      "Feature: 836, Score: -0.22178\n",
      "Feature: 837, Score: -0.34936\n",
      "Feature: 838, Score: -0.24269\n",
      "Feature: 839, Score: -0.22178\n",
      "Feature: 840, Score: -0.30304\n",
      "Feature: 841, Score: 0.38637\n",
      "Feature: 842, Score: -0.30304\n",
      "Feature: 843, Score: 0.32593\n",
      "Feature: 844, Score: 0.22831\n",
      "Feature: 845, Score: -0.30304\n",
      "Feature: 846, Score: 0.28366\n",
      "Feature: 847, Score: -0.30304\n",
      "Feature: 848, Score: -0.34936\n",
      "Feature: 849, Score: 0.32593\n",
      "Feature: 850, Score: -0.41564\n",
      "Feature: 851, Score: -0.41564\n",
      "Feature: 852, Score: -0.30304\n",
      "Feature: 853, Score: 0.28366\n",
      "Feature: 854, Score: 0.22831\n",
      "Feature: 855, Score: 0.32593\n",
      "Feature: 856, Score: 0.15051\n",
      "Feature: 857, Score: -0.34936\n",
      "Feature: 858, Score: 0.22831\n",
      "Feature: 859, Score: -0.22178\n",
      "Feature: 860, Score: 0.19323\n",
      "Feature: 861, Score: 0.25243\n",
      "Feature: 862, Score: -0.41564\n",
      "Feature: 863, Score: -0.34936\n",
      "Feature: 864, Score: -0.26894\n",
      "Feature: 865, Score: 0.25243\n",
      "Feature: 866, Score: 0.25243\n",
      "Feature: 867, Score: -0.26894\n",
      "Feature: 868, Score: -0.22178\n",
      "Feature: 869, Score: -0.24269\n",
      "Feature: 870, Score: -0.41564\n",
      "Feature: 871, Score: -0.34936\n",
      "Feature: 872, Score: -0.30304\n",
      "Feature: 873, Score: -0.30304\n",
      "Feature: 874, Score: -0.30304\n",
      "Feature: 875, Score: -0.26894\n",
      "Feature: 876, Score: 0.38637\n",
      "Feature: 877, Score: -0.41564\n",
      "Feature: 878, Score: -0.30304\n",
      "Feature: 879, Score: -0.34936\n",
      "Feature: 880, Score: -0.26894\n",
      "Feature: 881, Score: 0.16873\n",
      "Feature: 882, Score: 0.32593\n",
      "Feature: 883, Score: -0.41564\n",
      "Feature: 884, Score: -0.26894\n",
      "Feature: 885, Score: -0.41564\n",
      "Feature: 886, Score: -0.30304\n",
      "Feature: 887, Score: -0.24269\n",
      "Feature: 888, Score: 0.32593\n",
      "Feature: 889, Score: -0.34936\n",
      "Feature: 890, Score: -0.34936\n",
      "Feature: 891, Score: 0.22831\n",
      "Feature: 892, Score: 0.38637\n",
      "Feature: 893, Score: 0.32593\n",
      "Feature: 894, Score: 0.22831\n",
      "Feature: 895, Score: 0.16873\n",
      "Feature: 896, Score: -0.24269\n",
      "Feature: 897, Score: -0.30304\n",
      "Feature: 898, Score: -0.20467\n",
      "Feature: 899, Score: -0.22178\n",
      "Feature: 900, Score: 0.25243\n",
      "Feature: 901, Score: -0.20467\n",
      "Feature: 902, Score: -0.41564\n",
      "Feature: 903, Score: -0.12601\n",
      "Feature: 904, Score: -0.53788\n",
      "Feature: 905, Score: 0.28366\n",
      "Feature: 906, Score: -0.19038\n",
      "Feature: 907, Score: -0.30304\n",
      "Feature: 908, Score: 0.22831\n",
      "Feature: 909, Score: 0.10755\n",
      "Feature: 910, Score: -0.26894\n",
      "Feature: 911, Score: -0.34936\n",
      "Feature: 912, Score: 0.38637\n",
      "Feature: 913, Score: -0.26894\n",
      "Feature: 914, Score: 0.15900\n",
      "Feature: 915, Score: -0.15061\n",
      "Feature: 916, Score: -0.30304\n",
      "Feature: 917, Score: -0.41564\n",
      "Feature: 918, Score: -0.41564\n",
      "Feature: 919, Score: 0.38637\n",
      "Feature: 920, Score: -0.26894\n",
      "Feature: 921, Score: 0.14301\n",
      "Feature: 922, Score: 0.28366\n",
      "Feature: 923, Score: -0.14346\n",
      "Feature: 924, Score: 0.38637\n",
      "Feature: 925, Score: -0.34936\n",
      "Feature: 926, Score: -0.19038\n",
      "Feature: 927, Score: -0.30304\n",
      "Feature: 928, Score: -0.34936\n",
      "Feature: 929, Score: -0.26894\n",
      "Feature: 930, Score: 0.18000\n",
      "Feature: 931, Score: -0.24269\n",
      "Feature: 932, Score: 0.38637\n",
      "Feature: 933, Score: 0.38637\n",
      "Feature: 934, Score: -0.19038\n",
      "Feature: 935, Score: 0.32593\n",
      "Feature: 936, Score: 0.22831\n",
      "Feature: 937, Score: 0.38637\n",
      "Feature: 938, Score: -0.19038\n",
      "Feature: 939, Score: 0.18000\n",
      "Feature: 940, Score: -0.34936\n",
      "Feature: 941, Score: 0.28366\n",
      "Feature: 942, Score: 0.16873\n",
      "Feature: 943, Score: -0.41564\n",
      "Feature: 944, Score: 0.28366\n",
      "Feature: 945, Score: -0.41564\n",
      "Feature: 946, Score: -0.41564\n",
      "Feature: 947, Score: -0.26894\n",
      "Feature: 948, Score: 0.32593\n",
      "Feature: 949, Score: -0.20467\n",
      "Feature: 950, Score: -0.30304\n",
      "Feature: 951, Score: -0.26894\n",
      "Feature: 952, Score: -0.30304\n",
      "Feature: 953, Score: 0.38637\n",
      "Feature: 954, Score: -0.41564\n",
      "Feature: 955, Score: -0.24269\n",
      "Feature: 956, Score: -0.15865\n",
      "Feature: 957, Score: -0.26894\n",
      "Feature: 958, Score: 0.32593\n",
      "Feature: 959, Score: 0.38637\n",
      "Feature: 960, Score: -0.34936\n",
      "Feature: 961, Score: -0.17824\n",
      "Feature: 962, Score: 0.28366\n",
      "Feature: 963, Score: 0.18000\n",
      "Feature: 964, Score: -0.41564\n",
      "Feature: 965, Score: -0.41564\n",
      "Feature: 966, Score: 0.32593\n",
      "Feature: 967, Score: 0.25243\n",
      "Feature: 968, Score: -0.30304\n",
      "Feature: 969, Score: -0.22178\n",
      "Feature: 970, Score: -0.34936\n",
      "Feature: 971, Score: -0.41564\n",
      "Feature: 972, Score: 0.32593\n",
      "Feature: 973, Score: 0.38637\n",
      "Feature: 974, Score: 0.18000\n",
      "Feature: 975, Score: 0.19323\n",
      "Feature: 976, Score: 0.18000\n",
      "Feature: 977, Score: 0.18000\n",
      "Feature: 978, Score: -0.24269\n",
      "Feature: 979, Score: -0.24269\n",
      "Feature: 980, Score: 0.28366\n",
      "Feature: 981, Score: 0.15051\n",
      "Feature: 982, Score: -0.34936\n",
      "Feature: 983, Score: 0.19323\n",
      "Feature: 984, Score: -0.26894\n",
      "Feature: 985, Score: 0.25243\n",
      "Feature: 986, Score: -0.26894\n",
      "Feature: 987, Score: -0.30304\n",
      "Feature: 988, Score: 0.14301\n",
      "Feature: 989, Score: -0.26894\n",
      "Feature: 990, Score: -0.41564\n",
      "Feature: 991, Score: 0.32593\n",
      "Feature: 992, Score: -0.34936\n",
      "Feature: 993, Score: -0.30304\n",
      "Feature: 994, Score: -0.34936\n",
      "Feature: 995, Score: 0.19323\n",
      "Feature: 996, Score: -0.19038\n",
      "Feature: 997, Score: -0.41564\n",
      "Feature: 998, Score: 0.32593\n",
      "Feature: 999, Score: 0.32593\n",
      "Feature: 1000, Score: 0.25243\n",
      "Feature: 1001, Score: -0.34936\n",
      "Feature: 1002, Score: 0.38637\n",
      "Feature: 1003, Score: -0.41564\n",
      "Feature: 1004, Score: 0.28366\n",
      "Feature: 1005, Score: -0.34936\n",
      "Feature: 1006, Score: -0.41564\n",
      "Feature: 1007, Score: -0.30304\n",
      "Feature: 1008, Score: -0.34936\n",
      "Feature: 1009, Score: 0.15051\n",
      "Feature: 1010, Score: -0.41564\n",
      "Feature: 1011, Score: 0.28366\n",
      "Feature: 1012, Score: -0.26894\n",
      "Feature: 1013, Score: 0.15051\n",
      "Feature: 1014, Score: 0.38637\n",
      "Feature: 1015, Score: 0.18000\n",
      "Feature: 1016, Score: -0.34936\n",
      "Feature: 1017, Score: 0.19323\n",
      "Feature: 1018, Score: 0.15051\n",
      "Feature: 1019, Score: -0.22178\n",
      "Feature: 1020, Score: -0.34936\n",
      "Feature: 1021, Score: -0.22178\n",
      "Feature: 1022, Score: -0.19038\n",
      "Feature: 1023, Score: -0.24269\n",
      "Feature: 1024, Score: 0.32593\n",
      "Feature: 1025, Score: -0.30304\n",
      "Feature: 1026, Score: -0.34936\n",
      "Feature: 1027, Score: 0.38637\n",
      "Feature: 1028, Score: -0.41564\n",
      "Feature: 1029, Score: -0.26894\n",
      "Feature: 1030, Score: -0.34936\n",
      "Feature: 1031, Score: -0.12601\n",
      "Feature: 1032, Score: -0.41564\n",
      "Feature: 1033, Score: -0.30304\n",
      "Feature: 1034, Score: -0.30304\n",
      "Feature: 1035, Score: 0.22831\n",
      "Feature: 1036, Score: -0.34936\n",
      "Feature: 1037, Score: 0.18000\n",
      "Feature: 1038, Score: 0.28366\n",
      "Feature: 1039, Score: -0.34936\n",
      "Feature: 1040, Score: -0.34936\n",
      "Feature: 1041, Score: 0.22831\n",
      "Feature: 1042, Score: -0.26894\n",
      "Feature: 1043, Score: -0.40934\n",
      "Feature: 1044, Score: -0.34936\n",
      "Feature: 1045, Score: 0.22831\n",
      "Feature: 1046, Score: 0.32593\n",
      "Feature: 1047, Score: -0.34936\n",
      "Feature: 1048, Score: -0.20467\n",
      "Feature: 1049, Score: 0.32593\n",
      "Feature: 1050, Score: 0.38637\n",
      "Feature: 1051, Score: 0.38637\n",
      "Feature: 1052, Score: -0.20467\n",
      "Feature: 1053, Score: -0.34936\n",
      "Feature: 1054, Score: -0.30304\n",
      "Feature: 1055, Score: 0.18000\n",
      "Feature: 1056, Score: -0.19038\n",
      "Feature: 1057, Score: 0.28366\n",
      "Feature: 1058, Score: -0.24269\n",
      "Feature: 1059, Score: -0.34936\n",
      "Feature: 1060, Score: 0.38637\n",
      "Feature: 1061, Score: -0.34936\n",
      "Feature: 1062, Score: 0.32593\n",
      "Feature: 1063, Score: -0.28692\n",
      "Feature: 1064, Score: -0.41564\n",
      "Feature: 1065, Score: 0.16873\n",
      "Feature: 1066, Score: 0.22831\n",
      "Feature: 1067, Score: 0.20904\n",
      "Feature: 1068, Score: 0.28366\n",
      "Feature: 1069, Score: 0.14301\n",
      "Feature: 1070, Score: -0.34936\n",
      "Feature: 1071, Score: -0.30304\n",
      "Feature: 1072, Score: -0.34936\n",
      "Feature: 1073, Score: -0.41564\n",
      "Feature: 1074, Score: -0.30304\n",
      "Feature: 1075, Score: -0.20467\n",
      "Feature: 1076, Score: 0.20904\n",
      "Feature: 1077, Score: 0.19323\n",
      "Feature: 1078, Score: -0.34936\n",
      "Feature: 1079, Score: 0.18000\n",
      "Feature: 1080, Score: -0.22178\n",
      "Feature: 1081, Score: -0.24269\n",
      "Feature: 1082, Score: -0.24269\n",
      "Feature: 1083, Score: 0.32593\n",
      "Feature: 1084, Score: 0.32593\n",
      "Feature: 1085, Score: -0.34936\n",
      "Feature: 1086, Score: 0.38637\n",
      "Feature: 1087, Score: -0.30304\n",
      "Feature: 1088, Score: -0.30304\n",
      "Feature: 1089, Score: -0.22178\n",
      "Feature: 1090, Score: 0.19323\n",
      "Feature: 1091, Score: 0.21510\n",
      "Feature: 1092, Score: 0.28366\n",
      "Feature: 1093, Score: -0.33555\n",
      "Feature: 1094, Score: -0.30304\n",
      "Feature: 1095, Score: -0.22178\n",
      "Feature: 1096, Score: 0.25243\n",
      "Feature: 1097, Score: -0.30304\n",
      "Feature: 1098, Score: 0.18000\n",
      "Feature: 1099, Score: 0.50486\n",
      "Feature: 1100, Score: -0.41564\n",
      "Feature: 1101, Score: -0.24269\n",
      "Feature: 1102, Score: 0.32593\n",
      "Feature: 1103, Score: -0.41564\n",
      "Feature: 1104, Score: 0.22831\n",
      "Feature: 1105, Score: 0.20904\n",
      "Feature: 1106, Score: 0.14301\n",
      "Feature: 1107, Score: -0.30304\n",
      "Feature: 1108, Score: -0.34936\n",
      "Feature: 1109, Score: -0.34936\n",
      "Feature: 1110, Score: -0.41564\n",
      "Feature: 1111, Score: -0.30304\n",
      "Feature: 1112, Score: -0.41564\n",
      "Feature: 1113, Score: -0.41564\n",
      "Feature: 1114, Score: -0.41564\n",
      "Feature: 1115, Score: -0.19038\n",
      "Feature: 1116, Score: -0.19038\n",
      "Feature: 1117, Score: -0.41564\n",
      "Feature: 1118, Score: -0.34936\n",
      "Feature: 1119, Score: -0.15061\n",
      "Feature: 1120, Score: -0.34936\n",
      "Feature: 1121, Score: -0.41564\n",
      "Feature: 1122, Score: 0.22831\n",
      "Feature: 1123, Score: 0.38637\n",
      "Feature: 1124, Score: 0.18000\n",
      "Feature: 1125, Score: 0.28366\n",
      "Feature: 1126, Score: -0.41564\n",
      "Feature: 1127, Score: -0.19038\n",
      "Feature: 1128, Score: -0.34936\n",
      "Feature: 1129, Score: 0.25243\n",
      "Feature: 1130, Score: -0.24269\n",
      "Feature: 1131, Score: -0.30304\n",
      "Feature: 1132, Score: -0.41564\n",
      "Feature: 1133, Score: -0.26894\n",
      "Feature: 1134, Score: 0.18000\n",
      "Feature: 1135, Score: -0.24269\n",
      "Feature: 1136, Score: -0.35648\n",
      "Feature: 1137, Score: -0.34936\n",
      "Feature: 1138, Score: -0.30304\n",
      "Feature: 1139, Score: -0.19038\n",
      "Feature: 1140, Score: 0.22831\n",
      "Feature: 1141, Score: 0.32593\n",
      "Feature: 1142, Score: -0.34936\n",
      "Feature: 1143, Score: 0.32593\n",
      "Feature: 1144, Score: 0.28366\n",
      "Feature: 1145, Score: -0.41564\n",
      "Feature: 1146, Score: -0.34936\n",
      "Feature: 1147, Score: -0.34936\n",
      "Feature: 1148, Score: 0.22831\n",
      "Feature: 1149, Score: 0.32593\n",
      "Feature: 1150, Score: -0.30304\n",
      "Feature: 1151, Score: -0.16778\n",
      "Feature: 1152, Score: -0.13705\n",
      "Feature: 1153, Score: -0.26894\n",
      "Feature: 1154, Score: 0.25243\n",
      "Feature: 1155, Score: -0.19038\n",
      "Feature: 1156, Score: 0.28366\n",
      "Feature: 1157, Score: -0.26894\n",
      "Feature: 1158, Score: 0.28366\n",
      "Feature: 1159, Score: 0.38637\n",
      "Feature: 1160, Score: 0.32593\n",
      "Feature: 1161, Score: -0.34936\n",
      "Feature: 1162, Score: -0.41564\n",
      "Feature: 1163, Score: -0.15061\n",
      "Feature: 1164, Score: 0.19323\n",
      "Feature: 1165, Score: -0.22178\n",
      "Feature: 1166, Score: -0.34936\n",
      "Feature: 1167, Score: 0.32593\n",
      "Feature: 1168, Score: 0.25243\n",
      "Feature: 1169, Score: 0.32593\n",
      "Feature: 1170, Score: 0.15051\n",
      "Feature: 1171, Score: 0.18000\n",
      "Feature: 1172, Score: -0.41564\n",
      "Feature: 1173, Score: 0.32593\n",
      "Feature: 1174, Score: -0.22178\n",
      "Feature: 1175, Score: -0.19038\n",
      "Feature: 1176, Score: 0.38637\n",
      "Feature: 1177, Score: 0.22831\n",
      "Feature: 1178, Score: 0.38637\n",
      "Feature: 1179, Score: -0.20467\n",
      "Feature: 1180, Score: 0.25243\n",
      "Feature: 1181, Score: -0.19038\n",
      "Feature: 1182, Score: -0.41564\n",
      "Feature: 1183, Score: -0.26894\n",
      "Feature: 1184, Score: -0.30304\n",
      "Feature: 1185, Score: 0.32593\n",
      "Feature: 1186, Score: -0.34936\n",
      "Feature: 1187, Score: 0.22831\n",
      "Feature: 1188, Score: 0.38637\n",
      "Feature: 1189, Score: 0.19323\n",
      "Feature: 1190, Score: -0.26894\n",
      "Feature: 1191, Score: -0.34936\n",
      "Feature: 1192, Score: -0.41564\n",
      "Feature: 1193, Score: -0.19038\n",
      "Feature: 1194, Score: 0.32593\n",
      "Feature: 1195, Score: -0.26894\n",
      "Feature: 1196, Score: 0.19323\n",
      "Feature: 1197, Score: 0.25243\n",
      "Feature: 1198, Score: 0.25243\n",
      "Feature: 1199, Score: -0.22178\n",
      "Feature: 1200, Score: -0.19038\n",
      "Feature: 1201, Score: 0.19323\n",
      "Feature: 1202, Score: -0.24269\n",
      "Feature: 1203, Score: -0.24269\n",
      "Feature: 1204, Score: 0.28366\n",
      "Feature: 1205, Score: -0.41564\n",
      "Feature: 1206, Score: 0.28366\n",
      "Feature: 1207, Score: -0.24269\n",
      "Feature: 1208, Score: -0.41564\n",
      "Feature: 1209, Score: -0.26894\n",
      "Feature: 1210, Score: 0.25243\n",
      "Feature: 1211, Score: -0.16778\n",
      "Feature: 1212, Score: 0.32593\n",
      "Feature: 1213, Score: -0.24269\n",
      "Feature: 1214, Score: -0.41564\n",
      "Feature: 1215, Score: 0.32593\n",
      "Feature: 1216, Score: -0.30304\n",
      "Feature: 1217, Score: -0.34936\n",
      "Feature: 1218, Score: -0.22178\n",
      "Feature: 1219, Score: 0.38637\n",
      "Feature: 1220, Score: -0.19038\n",
      "Feature: 1221, Score: 0.20904\n",
      "Feature: 1222, Score: 0.38637\n",
      "Feature: 1223, Score: -0.20467\n",
      "Feature: 1224, Score: -0.41564\n",
      "Feature: 1225, Score: 0.28366\n",
      "Feature: 1226, Score: -0.38076\n",
      "Feature: 1227, Score: -0.26894\n",
      "Feature: 1228, Score: -0.24269\n",
      "Feature: 1229, Score: -0.24269\n",
      "Feature: 1230, Score: -0.26894\n",
      "Feature: 1231, Score: -0.34936\n",
      "Feature: 1232, Score: 0.15900\n",
      "Feature: 1233, Score: -0.24269\n",
      "Feature: 1234, Score: -0.41564\n",
      "Feature: 1235, Score: 0.38637\n",
      "Feature: 1236, Score: 0.38637\n",
      "Feature: 1237, Score: 0.38637\n",
      "Feature: 1238, Score: 0.14301\n",
      "Feature: 1239, Score: 0.38637\n",
      "Feature: 1240, Score: -0.15061\n",
      "Feature: 1241, Score: 0.32593\n",
      "Feature: 1242, Score: 0.38637\n",
      "Feature: 1243, Score: -0.34936\n",
      "Feature: 1244, Score: -0.30304\n",
      "Feature: 1245, Score: 0.18000\n",
      "Feature: 1246, Score: 0.18000\n",
      "Feature: 1247, Score: -0.30304\n",
      "Feature: 1248, Score: -0.19038\n",
      "Feature: 1249, Score: 0.16873\n",
      "Feature: 1250, Score: 0.25243\n",
      "Feature: 1251, Score: 0.38637\n",
      "Feature: 1252, Score: -0.26894\n",
      "Feature: 1253, Score: 0.15051\n",
      "Feature: 1254, Score: 0.25243\n",
      "Feature: 1255, Score: 0.25243\n",
      "Feature: 1256, Score: -0.15061\n",
      "Feature: 1257, Score: 0.32593\n",
      "Feature: 1258, Score: 0.38637\n",
      "Feature: 1259, Score: -0.26894\n",
      "Feature: 1260, Score: -0.41564\n",
      "Feature: 1261, Score: 0.32593\n",
      "Feature: 1262, Score: 0.32593\n",
      "Feature: 1263, Score: -0.53788\n",
      "Feature: 1264, Score: 0.32593\n",
      "Feature: 1265, Score: -0.24269\n",
      "Feature: 1266, Score: -0.41564\n",
      "Feature: 1267, Score: -0.41564\n",
      "Feature: 1268, Score: -0.19038\n",
      "Feature: 1269, Score: -0.19038\n",
      "Feature: 1270, Score: 0.10755\n",
      "Feature: 1271, Score: 0.22831\n",
      "Feature: 1272, Score: -0.41564\n",
      "Feature: 1273, Score: -0.34936\n",
      "Feature: 1274, Score: -0.34936\n",
      "Feature: 1275, Score: 0.28366\n",
      "Feature: 1276, Score: 0.32593\n",
      "Feature: 1277, Score: 0.32593\n",
      "Feature: 1278, Score: 0.20904\n",
      "Feature: 1279, Score: -0.20467\n",
      "Feature: 1280, Score: 0.22831\n",
      "Feature: 1281, Score: -0.15061\n",
      "Feature: 1282, Score: -0.17824\n",
      "Feature: 1283, Score: 0.32593\n",
      "Feature: 1284, Score: -0.15061\n",
      "Feature: 1285, Score: -0.26894\n",
      "Feature: 1286, Score: 0.32593\n",
      "Feature: 1287, Score: 0.28366\n",
      "Feature: 1288, Score: -0.34936\n",
      "Feature: 1289, Score: 0.38637\n",
      "Feature: 1290, Score: -0.26894\n",
      "Feature: 1291, Score: 0.38637\n",
      "Feature: 1292, Score: 0.32593\n",
      "Feature: 1293, Score: 0.22831\n",
      "Feature: 1294, Score: 0.16873\n",
      "Feature: 1295, Score: 0.32593\n",
      "Feature: 1296, Score: 0.13633\n",
      "Feature: 1297, Score: 0.32593\n",
      "Feature: 1298, Score: 0.38637\n",
      "Feature: 1299, Score: 0.38637\n",
      "Feature: 1300, Score: 0.20904\n",
      "Feature: 1301, Score: 0.32593\n",
      "Feature: 1302, Score: -0.14346\n",
      "Feature: 1303, Score: 0.38637\n",
      "Feature: 1304, Score: -0.15865\n",
      "Feature: 1305, Score: -0.26894\n",
      "Feature: 1306, Score: 0.25243\n",
      "Feature: 1307, Score: 0.32593\n",
      "Feature: 1308, Score: 0.38637\n",
      "Feature: 1309, Score: -0.22178\n",
      "Feature: 1310, Score: -0.33555\n",
      "Feature: 1311, Score: 0.32593\n",
      "Feature: 1312, Score: -0.30304\n",
      "Feature: 1313, Score: 0.28366\n",
      "Feature: 1314, Score: -0.34936\n",
      "Feature: 1315, Score: -0.34936\n",
      "Feature: 1316, Score: 0.22831\n",
      "Feature: 1317, Score: -0.41564\n",
      "Feature: 1318, Score: -0.34936\n",
      "Feature: 1319, Score: 0.32593\n",
      "Feature: 1320, Score: -0.22178\n",
      "Feature: 1321, Score: -0.20467\n",
      "Feature: 1322, Score: -0.20467\n",
      "Feature: 1323, Score: -0.34936\n",
      "Feature: 1324, Score: 0.28366\n",
      "Feature: 1325, Score: 0.38637\n",
      "Feature: 1326, Score: -0.34936\n",
      "Feature: 1327, Score: 0.25243\n",
      "Feature: 1328, Score: 0.14301\n",
      "Feature: 1329, Score: -0.34936\n",
      "Feature: 1330, Score: -0.41564\n",
      "Feature: 1331, Score: -0.30304\n",
      "Feature: 1332, Score: -0.22178\n",
      "Feature: 1333, Score: 0.32593\n",
      "Feature: 1334, Score: 0.22831\n",
      "Feature: 1335, Score: 0.32593\n",
      "Feature: 1336, Score: -0.41564\n",
      "Feature: 1337, Score: -0.26894\n",
      "Feature: 1338, Score: -0.41564\n",
      "Feature: 1339, Score: -0.34936\n",
      "Feature: 1340, Score: 0.38637\n",
      "Feature: 1341, Score: -0.41564\n",
      "Feature: 1342, Score: 0.28366\n",
      "Feature: 1343, Score: 0.28366\n",
      "Feature: 1344, Score: -0.26894\n",
      "Feature: 1345, Score: 0.28366\n",
      "Feature: 1346, Score: 0.20904\n",
      "Feature: 1347, Score: -0.26894\n",
      "Feature: 1348, Score: -0.24269\n",
      "Feature: 1349, Score: -0.15061\n",
      "Feature: 1350, Score: -0.41564\n",
      "Feature: 1351, Score: 0.32593\n",
      "Feature: 1352, Score: 0.10755\n",
      "Feature: 1353, Score: 0.16873\n",
      "Feature: 1354, Score: 0.28366\n",
      "Feature: 1355, Score: 0.28366\n",
      "Feature: 1356, Score: -0.34936\n",
      "Feature: 1357, Score: -0.20467\n",
      "Feature: 1358, Score: 0.32593\n",
      "Feature: 1359, Score: 0.32593\n",
      "Feature: 1360, Score: 0.38637\n",
      "Feature: 1361, Score: 0.18000\n",
      "Feature: 1362, Score: -0.26252\n",
      "Feature: 1363, Score: 0.20904\n",
      "Feature: 1364, Score: -0.41564\n",
      "Feature: 1365, Score: 0.28366\n",
      "Feature: 1366, Score: -0.24269\n",
      "Feature: 1367, Score: -0.34936\n",
      "Feature: 1368, Score: 0.25243\n",
      "Feature: 1369, Score: -0.41564\n",
      "Feature: 1370, Score: -0.20467\n",
      "Feature: 1371, Score: 0.16873\n",
      "Feature: 1372, Score: 0.15900\n",
      "Feature: 1373, Score: -0.24269\n",
      "Feature: 1374, Score: 0.38637\n",
      "Feature: 1375, Score: -0.34936\n",
      "Feature: 1376, Score: -0.41564\n",
      "Feature: 1377, Score: -0.41564\n",
      "Feature: 1378, Score: 0.28366\n",
      "Feature: 1379, Score: 0.25243\n",
      "Feature: 1380, Score: -0.30304\n",
      "Feature: 1381, Score: -0.24269\n",
      "Feature: 1382, Score: 0.19323\n",
      "Feature: 1383, Score: -0.34936\n",
      "Feature: 1384, Score: 0.25243\n",
      "Feature: 1385, Score: -0.41564\n",
      "Feature: 1386, Score: -0.19038\n",
      "Feature: 1387, Score: 0.38637\n",
      "Feature: 1388, Score: -0.15865\n",
      "Feature: 1389, Score: -0.34936\n",
      "Feature: 1390, Score: -0.34936\n",
      "Feature: 1391, Score: -0.13126\n",
      "Feature: 1392, Score: -0.24269\n",
      "Feature: 1393, Score: 0.15900\n",
      "Feature: 1394, Score: 0.38637\n",
      "Feature: 1395, Score: -0.30304\n",
      "Feature: 1396, Score: -0.19038\n",
      "Feature: 1397, Score: 0.28366\n",
      "Feature: 1398, Score: -0.30304\n",
      "Feature: 1399, Score: 0.38637\n",
      "Feature: 1400, Score: 0.32593\n",
      "Feature: 1401, Score: -0.30304\n",
      "Feature: 1402, Score: -0.30304\n",
      "Feature: 1403, Score: 0.28366\n",
      "Feature: 1404, Score: 0.38637\n",
      "Feature: 1405, Score: -0.26894\n",
      "Feature: 1406, Score: -0.15061\n",
      "Feature: 1407, Score: 0.32593\n",
      "Feature: 1408, Score: 0.18000\n",
      "Feature: 1409, Score: -0.15865\n",
      "Feature: 1410, Score: 0.32593\n",
      "Feature: 1411, Score: 0.32593\n",
      "Feature: 1412, Score: 0.32593\n",
      "Feature: 1413, Score: -0.20467\n",
      "Feature: 1414, Score: -0.20467\n",
      "Feature: 1415, Score: 0.32593\n",
      "Feature: 1416, Score: 0.32593\n",
      "Feature: 1417, Score: 0.38637\n",
      "Feature: 1418, Score: 0.25243\n",
      "Feature: 1419, Score: 0.22831\n",
      "Feature: 1420, Score: 0.38637\n",
      "Feature: 1421, Score: 0.32593\n",
      "Feature: 1422, Score: -0.34936\n",
      "Feature: 1423, Score: -0.34936\n",
      "Feature: 1424, Score: 0.39077\n",
      "Feature: 1425, Score: 0.18000\n",
      "Feature: 1426, Score: -0.24269\n",
      "Feature: 1427, Score: 0.32593\n",
      "Feature: 1428, Score: 0.19323\n",
      "Feature: 1429, Score: -0.30304\n",
      "Feature: 1430, Score: -0.26894\n",
      "Feature: 1431, Score: 0.13633\n",
      "Feature: 1432, Score: -0.30304\n",
      "Feature: 1433, Score: 0.22831\n",
      "Feature: 1434, Score: 0.28366\n",
      "Feature: 1435, Score: -0.30304\n",
      "Feature: 1436, Score: 0.38637\n",
      "Feature: 1437, Score: 0.19323\n",
      "Feature: 1438, Score: -0.41564\n",
      "Feature: 1439, Score: -0.30304\n",
      "Feature: 1440, Score: -0.41564\n",
      "Feature: 1441, Score: 0.32593\n",
      "Feature: 1442, Score: -0.26894\n",
      "Feature: 1443, Score: 0.18000\n",
      "Feature: 1444, Score: -0.20467\n",
      "Feature: 1445, Score: -0.22178\n",
      "Feature: 1446, Score: -0.34936\n",
      "Feature: 1447, Score: 0.15900\n",
      "Feature: 1448, Score: 0.38637\n",
      "Feature: 1449, Score: -0.41564\n",
      "Feature: 1450, Score: -0.34936\n",
      "Feature: 1451, Score: 0.15900\n",
      "Feature: 1452, Score: 0.38637\n",
      "Feature: 1453, Score: -0.30304\n",
      "Feature: 1454, Score: 0.38637\n",
      "Feature: 1455, Score: -0.34936\n",
      "Feature: 1456, Score: 0.15051\n",
      "Feature: 1457, Score: -0.15061\n",
      "Feature: 1458, Score: -0.22178\n",
      "Feature: 1459, Score: -0.22178\n",
      "Feature: 1460, Score: -0.22178\n",
      "Feature: 1461, Score: 0.38637\n",
      "Feature: 1462, Score: -0.13705\n",
      "Feature: 1463, Score: 0.22831\n",
      "Feature: 1464, Score: 0.38637\n",
      "Feature: 1465, Score: -0.22178\n",
      "Feature: 1466, Score: -0.30304\n",
      "Feature: 1467, Score: 0.32593\n",
      "Feature: 1468, Score: 0.16873\n",
      "Feature: 1469, Score: 0.38637\n",
      "Feature: 1470, Score: 0.45662\n",
      "Feature: 1471, Score: -0.24269\n",
      "Feature: 1472, Score: -0.26894\n",
      "Feature: 1473, Score: 0.16873\n",
      "Feature: 1474, Score: 0.16873\n",
      "Feature: 1475, Score: 0.16873\n",
      "Feature: 1476, Score: -0.30304\n",
      "Feature: 1477, Score: -0.34936\n",
      "Feature: 1478, Score: -0.30304\n",
      "Feature: 1479, Score: 0.28366\n",
      "Feature: 1480, Score: -0.24269\n",
      "Feature: 1481, Score: -0.20467\n",
      "Feature: 1482, Score: 0.28366\n",
      "Feature: 1483, Score: 0.28366\n",
      "Feature: 1484, Score: -0.41564\n",
      "Feature: 1485, Score: -0.20467\n",
      "Feature: 1486, Score: 0.25243\n",
      "Feature: 1487, Score: 0.22831\n",
      "Feature: 1488, Score: -0.30304\n",
      "Feature: 1489, Score: 0.33746\n",
      "Feature: 1490, Score: 0.38637\n",
      "Feature: 1491, Score: -0.34936\n",
      "Feature: 1492, Score: -0.26894\n",
      "Feature: 1493, Score: -0.19038\n",
      "Feature: 1494, Score: 0.16873\n",
      "Feature: 1495, Score: -0.41564\n",
      "Feature: 1496, Score: 0.32593\n",
      "Feature: 1497, Score: 0.22831\n",
      "Feature: 1498, Score: 0.25243\n",
      "Feature: 1499, Score: 0.38646\n",
      "Feature: 1500, Score: -0.20467\n",
      "Feature: 1501, Score: 0.38637\n",
      "Feature: 1502, Score: 0.22831\n",
      "Feature: 1503, Score: 0.22831\n",
      "Feature: 1504, Score: -0.24269\n",
      "Feature: 1505, Score: -0.26894\n",
      "Feature: 1506, Score: -0.41564\n",
      "Feature: 1507, Score: -0.19038\n",
      "Feature: 1508, Score: 0.28366\n",
      "Feature: 1509, Score: -0.30304\n",
      "Feature: 1510, Score: -0.22178\n",
      "Feature: 1511, Score: 0.32593\n",
      "Feature: 1512, Score: 0.32593\n",
      "Feature: 1513, Score: 0.38637\n",
      "Feature: 1514, Score: -0.30304\n",
      "Feature: 1515, Score: 0.20904\n",
      "Feature: 1516, Score: 0.28366\n",
      "Feature: 1517, Score: -0.34936\n",
      "Feature: 1518, Score: -0.22178\n",
      "Feature: 1519, Score: -0.34936\n",
      "Feature: 1520, Score: 0.15900\n",
      "Feature: 1521, Score: 0.38637\n",
      "Feature: 1522, Score: -0.26894\n",
      "Feature: 1523, Score: 0.38637\n",
      "Feature: 1524, Score: 0.38637\n",
      "Feature: 1525, Score: 0.38637\n",
      "Feature: 1526, Score: 0.38637\n",
      "Feature: 1527, Score: 0.25243\n",
      "Feature: 1528, Score: 0.28366\n",
      "Feature: 1529, Score: -0.30304\n",
      "Feature: 1530, Score: -0.24269\n",
      "Feature: 1531, Score: 0.14301\n",
      "Feature: 1532, Score: -0.24269\n",
      "Feature: 1533, Score: -0.24269\n",
      "Feature: 1534, Score: 0.18000\n",
      "Feature: 1535, Score: -0.30304\n",
      "Feature: 1536, Score: -0.34936\n",
      "Feature: 1537, Score: -0.41564\n",
      "Feature: 1538, Score: 0.22831\n",
      "Feature: 1539, Score: 0.22831\n",
      "Feature: 1540, Score: -0.26894\n",
      "Feature: 1541, Score: -0.15061\n",
      "Feature: 1542, Score: -0.26894\n",
      "Feature: 1543, Score: -0.41564\n",
      "Feature: 1544, Score: 0.19323\n",
      "Feature: 1545, Score: -0.24269\n",
      "Feature: 1546, Score: -0.30304\n",
      "Feature: 1547, Score: -0.15865\n",
      "Feature: 1548, Score: -0.31731\n",
      "Feature: 1549, Score: -0.12601\n",
      "Feature: 1550, Score: 0.32593\n",
      "Feature: 1551, Score: -0.30304\n",
      "Feature: 1552, Score: 0.28366\n",
      "Feature: 1553, Score: 0.25243\n",
      "Feature: 1554, Score: 0.38637\n",
      "Feature: 1555, Score: -0.34936\n",
      "Feature: 1556, Score: 0.16873\n",
      "Feature: 1557, Score: -0.34936\n",
      "Feature: 1558, Score: -0.17824\n",
      "Feature: 1559, Score: -0.41564\n",
      "Feature: 1560, Score: -0.20467\n",
      "Feature: 1561, Score: -0.20467\n",
      "Feature: 1562, Score: -0.19038\n",
      "Feature: 1563, Score: 0.18000\n",
      "Feature: 1564, Score: 0.25243\n",
      "Feature: 1565, Score: -0.34936\n",
      "Feature: 1566, Score: 0.22831\n",
      "Feature: 1567, Score: -0.30304\n",
      "Feature: 1568, Score: -0.41564\n",
      "Feature: 1569, Score: -0.41564\n",
      "Feature: 1570, Score: -0.30304\n",
      "Feature: 1571, Score: -0.17824\n",
      "Feature: 1572, Score: 0.38637\n",
      "Feature: 1573, Score: -0.20467\n",
      "Feature: 1574, Score: -0.20467\n",
      "Feature: 1575, Score: 0.38637\n",
      "Feature: 1576, Score: -0.34936\n",
      "Feature: 1577, Score: 0.18000\n",
      "Feature: 1578, Score: -0.30304\n",
      "Feature: 1579, Score: 0.20904\n",
      "Feature: 1580, Score: 0.38637\n",
      "Feature: 1581, Score: 0.18000\n",
      "Feature: 1582, Score: 0.18000\n",
      "Feature: 1583, Score: -0.24269\n",
      "Feature: 1584, Score: -0.20467\n",
      "Feature: 1585, Score: -0.22178\n",
      "Feature: 1586, Score: -0.39379\n",
      "Feature: 1587, Score: -0.30304\n",
      "Feature: 1588, Score: 0.32593\n",
      "Feature: 1589, Score: -0.30304\n",
      "Feature: 1590, Score: 0.15900\n",
      "Feature: 1591, Score: 0.32593\n",
      "Feature: 1592, Score: 0.38637\n",
      "Feature: 1593, Score: 0.28366\n",
      "Feature: 1594, Score: 0.38637\n",
      "Feature: 1595, Score: -0.24269\n",
      "Feature: 1596, Score: -0.26894\n",
      "Feature: 1597, Score: -0.24269\n",
      "Feature: 1598, Score: 0.32593\n",
      "Feature: 1599, Score: -0.26894\n",
      "Feature: 1600, Score: 0.32593\n",
      "Feature: 1601, Score: -0.19038\n",
      "Feature: 1602, Score: -0.17824\n",
      "Feature: 1603, Score: 0.32593\n",
      "Feature: 1604, Score: 0.32593\n",
      "Feature: 1605, Score: -0.34936\n",
      "Feature: 1606, Score: 0.38637\n",
      "Feature: 1607, Score: -0.30304\n",
      "Feature: 1608, Score: 0.22831\n",
      "Feature: 1609, Score: -0.41564\n",
      "Feature: 1610, Score: -0.34936\n",
      "Feature: 1611, Score: -0.14346\n",
      "Feature: 1612, Score: -0.30304\n",
      "Feature: 1613, Score: -0.22178\n",
      "Feature: 1614, Score: 0.15051\n",
      "Feature: 1615, Score: -0.24269\n",
      "Feature: 1616, Score: 0.38637\n",
      "Feature: 1617, Score: -0.34936\n",
      "Feature: 1618, Score: 0.18000\n",
      "Feature: 1619, Score: 0.22831\n",
      "Feature: 1620, Score: -0.24269\n",
      "Feature: 1621, Score: -0.26894\n",
      "Feature: 1622, Score: -0.30122\n",
      "Feature: 1623, Score: 0.15051\n",
      "Feature: 1624, Score: 0.32593\n",
      "Feature: 1625, Score: 0.15900\n",
      "Feature: 1626, Score: -0.34936\n",
      "Feature: 1627, Score: -0.26894\n",
      "Feature: 1628, Score: 0.20904\n",
      "Feature: 1629, Score: 0.41808\n",
      "Feature: 1630, Score: -0.30304\n",
      "Feature: 1631, Score: -0.30304\n",
      "Feature: 1632, Score: 0.32593\n",
      "Feature: 1633, Score: -0.34936\n",
      "Feature: 1634, Score: 0.18000\n",
      "Feature: 1635, Score: 0.28366\n",
      "Feature: 1636, Score: 0.14301\n",
      "Feature: 1637, Score: -0.26894\n",
      "Feature: 1638, Score: -0.30304\n",
      "Feature: 1639, Score: 0.28366\n",
      "Feature: 1640, Score: -0.30304\n",
      "Feature: 1641, Score: -0.19038\n",
      "Feature: 1642, Score: 0.28366\n",
      "Feature: 1643, Score: 0.28366\n",
      "Feature: 1644, Score: 0.28366\n",
      "Feature: 1645, Score: -0.19038\n",
      "Feature: 1646, Score: -0.24269\n",
      "Feature: 1647, Score: 0.28366\n",
      "Feature: 1648, Score: -0.41564\n",
      "Feature: 1649, Score: -0.26894\n",
      "Feature: 1650, Score: 0.10755\n",
      "Feature: 1651, Score: 0.38637\n",
      "Feature: 1652, Score: 0.15051\n",
      "Feature: 1653, Score: 0.38637\n",
      "Feature: 1654, Score: -0.24269\n",
      "Feature: 1655, Score: -0.34936\n",
      "Feature: 1656, Score: -0.26894\n",
      "Feature: 1657, Score: -0.34936\n",
      "Feature: 1658, Score: -0.30304\n",
      "Feature: 1659, Score: 0.28366\n",
      "Feature: 1660, Score: 0.32593\n",
      "Feature: 1661, Score: 0.28366\n",
      "Feature: 1662, Score: 0.25243\n",
      "Feature: 1663, Score: -0.41564\n",
      "Feature: 1664, Score: -0.34936\n",
      "Feature: 1665, Score: 0.20904\n",
      "Feature: 1666, Score: 0.22831\n",
      "Feature: 1667, Score: -0.30304\n",
      "Feature: 1668, Score: -0.41564\n",
      "Feature: 1669, Score: -0.34936\n",
      "Feature: 1670, Score: -0.34936\n",
      "Feature: 1671, Score: 0.38637\n",
      "Feature: 1672, Score: 0.32593\n",
      "Feature: 1673, Score: 0.38637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 1674, Score: 0.25243\n",
      "Feature: 1675, Score: 0.19323\n",
      "Feature: 1676, Score: -0.34936\n",
      "Feature: 1677, Score: -0.22178\n",
      "Feature: 1678, Score: -0.15061\n",
      "Feature: 1679, Score: 0.18000\n",
      "Feature: 1680, Score: -0.26894\n",
      "Feature: 1681, Score: -0.19038\n",
      "Feature: 1682, Score: -0.24269\n",
      "Feature: 1683, Score: -0.19038\n",
      "Feature: 1684, Score: -0.34936\n",
      "Feature: 1685, Score: -0.15061\n",
      "Feature: 1686, Score: -0.41564\n",
      "Feature: 1687, Score: 0.32593\n",
      "Feature: 1688, Score: 0.22831\n",
      "Feature: 1689, Score: 0.38637\n",
      "Feature: 1690, Score: 0.32593\n",
      "Feature: 1691, Score: 0.22831\n",
      "Feature: 1692, Score: -0.34936\n",
      "Feature: 1693, Score: -0.30304\n",
      "Feature: 1694, Score: 0.38637\n",
      "Feature: 1695, Score: -0.19038\n",
      "Feature: 1696, Score: 0.25243\n",
      "Feature: 1697, Score: 0.13633\n",
      "Feature: 1698, Score: -0.30304\n",
      "Feature: 1699, Score: -0.26894\n",
      "Feature: 1700, Score: 0.38637\n",
      "Feature: 1701, Score: -0.26894\n",
      "Feature: 1702, Score: -0.26894\n",
      "Feature: 1703, Score: -0.15061\n",
      "Feature: 1704, Score: -0.19038\n",
      "Feature: 1705, Score: -0.34936\n",
      "Feature: 1706, Score: -0.15061\n",
      "Feature: 1707, Score: 0.28366\n",
      "Feature: 1708, Score: 0.38637\n",
      "Feature: 1709, Score: -0.26894\n",
      "Feature: 1710, Score: -0.26894\n",
      "Feature: 1711, Score: -0.41564\n",
      "Feature: 1712, Score: -0.41564\n",
      "Feature: 1713, Score: -0.34936\n",
      "Feature: 1714, Score: -0.34936\n",
      "Feature: 1715, Score: 0.22831\n",
      "Feature: 1716, Score: -0.41564\n",
      "Feature: 1717, Score: 0.32593\n",
      "Feature: 1718, Score: 0.32593\n",
      "Feature: 1719, Score: -0.41564\n",
      "Feature: 1720, Score: -0.19038\n",
      "Feature: 1721, Score: -0.24269\n",
      "Feature: 1722, Score: 0.16873\n",
      "Feature: 1723, Score: 0.15051\n",
      "Feature: 1724, Score: -0.19038\n",
      "Feature: 1725, Score: 0.38637\n",
      "Feature: 1726, Score: -0.34936\n",
      "Feature: 1727, Score: -0.41564\n",
      "Feature: 1728, Score: -0.26894\n",
      "Feature: 1729, Score: -0.20467\n",
      "Feature: 1730, Score: -0.15061\n",
      "Feature: 1731, Score: -0.34936\n",
      "Feature: 1732, Score: -0.24269\n",
      "Feature: 1733, Score: -0.24269\n",
      "Feature: 1734, Score: -0.30304\n",
      "Feature: 1735, Score: 0.28366\n",
      "Feature: 1736, Score: 0.32593\n",
      "Feature: 1737, Score: 0.38637\n",
      "Feature: 1738, Score: 0.14301\n",
      "Feature: 1739, Score: -0.26894\n",
      "Feature: 1740, Score: 0.32593\n",
      "Feature: 1741, Score: -0.34936\n",
      "Feature: 1742, Score: 0.15900\n",
      "Feature: 1743, Score: 0.38637\n",
      "Feature: 1744, Score: 0.22831\n",
      "Feature: 1745, Score: -0.34936\n",
      "Feature: 1746, Score: -0.41564\n",
      "Feature: 1747, Score: 0.32593\n",
      "Feature: 1748, Score: 0.38637\n",
      "Feature: 1749, Score: -0.15061\n",
      "Feature: 1750, Score: 0.20904\n",
      "Feature: 1751, Score: -0.30304\n",
      "Feature: 1752, Score: 0.32593\n",
      "Feature: 1753, Score: 0.32593\n",
      "Feature: 1754, Score: -0.26252\n",
      "Feature: 1755, Score: 0.25243\n",
      "Feature: 1756, Score: -0.34936\n",
      "Feature: 1757, Score: 0.13633\n",
      "Feature: 1758, Score: 0.38637\n",
      "Feature: 1759, Score: 0.38637\n",
      "Feature: 1760, Score: 0.22831\n",
      "Feature: 1761, Score: -0.30304\n",
      "Feature: 1762, Score: 0.25243\n",
      "Feature: 1763, Score: 0.38637\n",
      "Feature: 1764, Score: 0.16873\n",
      "Feature: 1765, Score: -0.30304\n",
      "Feature: 1766, Score: 0.32593\n",
      "Feature: 1767, Score: 0.32593\n",
      "Feature: 1768, Score: -0.30304\n",
      "Feature: 1769, Score: -0.34936\n",
      "Feature: 1770, Score: 0.38637\n",
      "Feature: 1771, Score: 0.22831\n",
      "Feature: 1772, Score: 0.19323\n",
      "Feature: 1773, Score: 0.38637\n",
      "Feature: 1774, Score: -0.19038\n",
      "Feature: 1775, Score: 0.09769\n",
      "Feature: 1776, Score: -0.30304\n",
      "Feature: 1777, Score: -0.26894\n",
      "Feature: 1778, Score: -0.26894\n",
      "Feature: 1779, Score: -0.30304\n",
      "Feature: 1780, Score: -0.15061\n",
      "Feature: 1781, Score: -0.30304\n",
      "Feature: 1782, Score: -0.34936\n",
      "Feature: 1783, Score: -0.41564\n",
      "Feature: 1784, Score: 0.22831\n",
      "Feature: 1785, Score: -0.22178\n",
      "Feature: 1786, Score: -0.13705\n",
      "Feature: 1787, Score: -0.30304\n",
      "Feature: 1788, Score: -0.34936\n",
      "Feature: 1789, Score: 0.38637\n",
      "Feature: 1790, Score: 0.38637\n",
      "Feature: 1791, Score: 0.32593\n",
      "Feature: 1792, Score: 0.25243\n",
      "Feature: 1793, Score: -0.12601\n",
      "Feature: 1794, Score: 0.38637\n",
      "Feature: 1795, Score: 0.32593\n",
      "Feature: 1796, Score: 0.32593\n",
      "Feature: 1797, Score: 0.28366\n",
      "Feature: 1798, Score: -0.34936\n",
      "Feature: 1799, Score: -0.24269\n",
      "Feature: 1800, Score: -0.30304\n",
      "Feature: 1801, Score: -0.30304\n",
      "Feature: 1802, Score: -0.41564\n",
      "Feature: 1803, Score: -0.41564\n",
      "Feature: 1804, Score: 0.32593\n",
      "Feature: 1805, Score: 0.32593\n",
      "Feature: 1806, Score: -0.24269\n",
      "Feature: 1807, Score: -0.41564\n",
      "Feature: 1808, Score: 0.22831\n",
      "Feature: 1809, Score: -0.41564\n",
      "Feature: 1810, Score: -0.30304\n",
      "Feature: 1811, Score: -0.34936\n",
      "Feature: 1812, Score: 0.18000\n",
      "Feature: 1813, Score: -0.41564\n",
      "Feature: 1814, Score: -0.26894\n",
      "Feature: 1815, Score: -0.34936\n",
      "Feature: 1816, Score: 0.22831\n",
      "Feature: 1817, Score: -0.24269\n",
      "Feature: 1818, Score: -0.13705\n",
      "Feature: 1819, Score: -0.34936\n",
      "Feature: 1820, Score: -0.30304\n",
      "Feature: 1821, Score: 0.18000\n",
      "Feature: 1822, Score: 0.38637\n",
      "Feature: 1823, Score: -0.24269\n",
      "Feature: 1824, Score: 0.22831\n",
      "Feature: 1825, Score: -0.24269\n",
      "Feature: 1826, Score: 0.22831\n",
      "Feature: 1827, Score: -0.24269\n",
      "Feature: 1828, Score: -0.15061\n",
      "Feature: 1829, Score: 0.22831\n",
      "Feature: 1830, Score: -0.41564\n",
      "Feature: 1831, Score: 0.22831\n",
      "Feature: 1832, Score: 0.13633\n",
      "Feature: 1833, Score: -0.34936\n",
      "Feature: 1834, Score: 0.38637\n",
      "Feature: 1835, Score: -0.19038\n",
      "Feature: 1836, Score: -0.34936\n",
      "Feature: 1837, Score: -0.30304\n",
      "Feature: 1838, Score: -0.44356\n",
      "Feature: 1839, Score: 0.28366\n",
      "Feature: 1840, Score: 0.38637\n",
      "Feature: 1841, Score: -0.41564\n",
      "Feature: 1842, Score: 0.25243\n",
      "Feature: 1843, Score: -0.41564\n",
      "Feature: 1844, Score: -0.26894\n",
      "Feature: 1845, Score: 0.28366\n",
      "Feature: 1846, Score: 0.18000\n",
      "Feature: 1847, Score: 0.38637\n",
      "Feature: 1848, Score: -0.15061\n",
      "Feature: 1849, Score: 0.32593\n",
      "Feature: 1850, Score: -0.24269\n",
      "Feature: 1851, Score: 0.36000\n",
      "Feature: 1852, Score: -0.41564\n",
      "Feature: 1853, Score: -0.41564\n",
      "Feature: 1854, Score: 0.38637\n",
      "Feature: 1855, Score: -0.30304\n",
      "Feature: 1856, Score: -0.20467\n",
      "Feature: 1857, Score: 0.18000\n",
      "Feature: 1858, Score: 0.14301\n",
      "Feature: 1859, Score: -0.20467\n",
      "Feature: 1860, Score: -0.34936\n",
      "Feature: 1861, Score: 0.38637\n",
      "Feature: 1862, Score: -0.24269\n",
      "Feature: 1863, Score: -0.26894\n",
      "Feature: 1864, Score: 0.28366\n",
      "Feature: 1865, Score: -0.17824\n",
      "Feature: 1866, Score: 0.32593\n",
      "Feature: 1867, Score: -0.30304\n",
      "Feature: 1868, Score: -0.24269\n",
      "Feature: 1869, Score: 0.13034\n",
      "Feature: 1870, Score: -0.41564\n",
      "Feature: 1871, Score: -0.30304\n",
      "Feature: 1872, Score: -0.30304\n",
      "Feature: 1873, Score: -0.26894\n",
      "Feature: 1874, Score: 0.18000\n",
      "Feature: 1875, Score: -0.34936\n",
      "Feature: 1876, Score: -0.22178\n",
      "Feature: 1877, Score: -0.34936\n",
      "Feature: 1878, Score: -0.41564\n",
      "Feature: 1879, Score: -0.20467\n",
      "Feature: 1880, Score: 0.28366\n",
      "Feature: 1881, Score: -0.24269\n",
      "Feature: 1882, Score: -0.24269\n",
      "Feature: 1883, Score: -0.34936\n",
      "Feature: 1884, Score: 0.38637\n",
      "Feature: 1885, Score: 0.38637\n",
      "Feature: 1886, Score: 0.32593\n",
      "Feature: 1887, Score: 0.25243\n",
      "Feature: 1888, Score: 0.19323\n",
      "Feature: 1889, Score: -0.41564\n",
      "Feature: 1890, Score: 0.18000\n",
      "Feature: 1891, Score: -0.30304\n",
      "Feature: 1892, Score: -0.34936\n",
      "Feature: 1893, Score: -0.30304\n",
      "Feature: 1894, Score: -0.41564\n",
      "Feature: 1895, Score: 0.15900\n",
      "Feature: 1896, Score: -0.19038\n",
      "Feature: 1897, Score: 0.19323\n",
      "Feature: 1898, Score: -0.19038\n",
      "Feature: 1899, Score: 0.18000\n",
      "Feature: 1900, Score: 0.38637\n",
      "Feature: 1901, Score: -0.30304\n",
      "Feature: 1902, Score: -0.34936\n",
      "Feature: 1903, Score: -0.13705\n",
      "Feature: 1904, Score: 0.18000\n",
      "Feature: 1905, Score: -0.24269\n",
      "Feature: 1906, Score: -0.30304\n",
      "Feature: 1907, Score: -0.24269\n",
      "Feature: 1908, Score: 0.28366\n",
      "Feature: 1909, Score: -0.41564\n",
      "Feature: 1910, Score: -0.30304\n",
      "Feature: 1911, Score: -0.34936\n",
      "Feature: 1912, Score: 0.25243\n",
      "Feature: 1913, Score: -0.26894\n",
      "Feature: 1914, Score: 0.28366\n",
      "Feature: 1915, Score: 0.28366\n",
      "Feature: 1916, Score: -0.41564\n",
      "Feature: 1917, Score: 0.38637\n",
      "Feature: 1918, Score: 0.25243\n",
      "Feature: 1919, Score: -0.30304\n",
      "Feature: 1920, Score: -0.14346\n",
      "Feature: 1921, Score: -0.34936\n",
      "Feature: 1922, Score: -0.22178\n",
      "Feature: 1923, Score: 0.38637\n",
      "Feature: 1924, Score: 0.15900\n",
      "Feature: 1925, Score: 0.28366\n",
      "Feature: 1926, Score: -0.26894\n",
      "Feature: 1927, Score: 0.25243\n",
      "Feature: 1928, Score: -0.34936\n",
      "Feature: 1929, Score: -0.41564\n",
      "Feature: 1930, Score: 0.15900\n",
      "Feature: 1931, Score: 0.15900\n",
      "Feature: 1932, Score: 0.18000\n",
      "Feature: 1933, Score: 0.18000\n",
      "Feature: 1934, Score: -0.41564\n",
      "Feature: 1935, Score: -0.30304\n",
      "Feature: 1936, Score: 0.32593\n",
      "Feature: 1937, Score: -0.30304\n",
      "Feature: 1938, Score: 0.20904\n",
      "Feature: 1939, Score: -0.30304\n",
      "Feature: 1940, Score: -0.26894\n",
      "Feature: 1941, Score: -0.41564\n",
      "Feature: 1942, Score: -0.20467\n",
      "Feature: 1943, Score: -0.34936\n",
      "Feature: 1944, Score: 0.38637\n",
      "Feature: 1945, Score: 0.18000\n",
      "Feature: 1946, Score: -0.19038\n",
      "Feature: 1947, Score: 0.28366\n",
      "Feature: 1948, Score: 0.25243\n",
      "Feature: 1949, Score: -0.20467\n",
      "Feature: 1950, Score: 0.18000\n",
      "Feature: 1951, Score: 0.28366\n",
      "Feature: 1952, Score: 0.50486\n",
      "Feature: 1953, Score: -0.24269\n",
      "Feature: 1954, Score: -0.17824\n",
      "Feature: 1955, Score: -0.30304\n",
      "Feature: 1956, Score: 0.32593\n",
      "Feature: 1957, Score: 0.32593\n",
      "Feature: 1958, Score: 0.15051\n",
      "Feature: 1959, Score: 0.32593\n",
      "Feature: 1960, Score: 0.20904\n",
      "Feature: 1961, Score: 0.38637\n",
      "Feature: 1962, Score: 0.32593\n",
      "Feature: 1963, Score: 0.22831\n",
      "Feature: 1964, Score: 0.22831\n",
      "Feature: 1965, Score: 0.15051\n",
      "Feature: 1966, Score: 0.38637\n",
      "Feature: 1967, Score: -0.34936\n",
      "Feature: 1968, Score: 0.38637\n",
      "Feature: 1969, Score: 0.32593\n",
      "Feature: 1970, Score: -0.30304\n",
      "Feature: 1971, Score: 0.15900\n",
      "Feature: 1972, Score: 0.32593\n",
      "Feature: 1973, Score: -0.30304\n",
      "Feature: 1974, Score: 0.32593\n",
      "Feature: 1975, Score: -0.41564\n",
      "Feature: 1976, Score: 0.22831\n",
      "Feature: 1977, Score: 0.25243\n",
      "Feature: 1978, Score: 0.38637\n",
      "Feature: 1979, Score: -0.34936\n",
      "Feature: 1980, Score: 0.38637\n",
      "Feature: 1981, Score: -0.41564\n",
      "Feature: 1982, Score: -0.34936\n",
      "Feature: 1983, Score: -0.41564\n",
      "Feature: 1984, Score: 0.25243\n",
      "Feature: 1985, Score: -0.24269\n",
      "Feature: 1986, Score: -0.26894\n",
      "Feature: 1987, Score: -0.17824\n",
      "Feature: 1988, Score: 0.38637\n",
      "Feature: 1989, Score: 0.38637\n",
      "Feature: 1990, Score: 0.38637\n",
      "Feature: 1991, Score: -0.24269\n",
      "Feature: 1992, Score: 0.31801\n",
      "Feature: 1993, Score: -0.30304\n",
      "Feature: 1994, Score: 0.15051\n",
      "Feature: 1995, Score: 0.38637\n",
      "Feature: 1996, Score: 0.38637\n",
      "Feature: 1997, Score: 0.09769\n",
      "Feature: 1998, Score: -0.30304\n",
      "Feature: 1999, Score: 0.25243\n",
      "Feature: 2000, Score: 0.18000\n",
      "Feature: 2001, Score: -0.20467\n",
      "Feature: 2002, Score: -0.20467\n",
      "Feature: 2003, Score: 0.25243\n",
      "Feature: 2004, Score: 0.38637\n",
      "Feature: 2005, Score: -0.30304\n",
      "Feature: 2006, Score: -0.26894\n",
      "Feature: 2007, Score: 0.14301\n",
      "Feature: 2008, Score: 0.20904\n",
      "Feature: 2009, Score: -0.30304\n",
      "Feature: 2010, Score: -0.34936\n",
      "Feature: 2011, Score: 0.28366\n",
      "Feature: 2012, Score: 0.16873\n",
      "Feature: 2013, Score: -0.34936\n",
      "Feature: 2014, Score: -0.34936\n",
      "Feature: 2015, Score: -0.24269\n",
      "Feature: 2016, Score: 0.38637\n",
      "Feature: 2017, Score: -0.41564\n",
      "Feature: 2018, Score: -0.41564\n",
      "Feature: 2019, Score: -0.34936\n",
      "Feature: 2020, Score: 0.21510\n",
      "Feature: 2021, Score: -0.30304\n",
      "Feature: 2022, Score: -0.24269\n",
      "Feature: 2023, Score: 0.25243\n",
      "Feature: 2024, Score: 0.38637\n",
      "Feature: 2025, Score: 0.13633\n",
      "Feature: 2026, Score: -0.34936\n",
      "Feature: 2027, Score: 0.38637\n",
      "Feature: 2028, Score: 0.38637\n",
      "Feature: 2029, Score: -0.13126\n",
      "Feature: 2030, Score: 0.25243\n",
      "Feature: 2031, Score: 0.32593\n",
      "Feature: 2032, Score: -0.15865\n",
      "Feature: 2033, Score: -0.34936\n",
      "Feature: 2034, Score: 0.28366\n",
      "Feature: 2035, Score: 0.28366\n",
      "Feature: 2036, Score: 0.14301\n",
      "Feature: 2037, Score: -0.26894\n",
      "Feature: 2038, Score: 0.22831\n",
      "Feature: 2039, Score: 0.38637\n",
      "Feature: 2040, Score: -0.34936\n",
      "Feature: 2041, Score: 0.32593\n",
      "Feature: 2042, Score: 0.25243\n",
      "Feature: 2043, Score: -0.17824\n",
      "Feature: 2044, Score: -0.41564\n",
      "Feature: 2045, Score: -0.24269\n",
      "Feature: 2046, Score: -0.41564\n",
      "Feature: 2047, Score: -0.26894\n",
      "Feature: 2048, Score: -0.20467\n",
      "Feature: 2049, Score: -0.30304\n",
      "Feature: 2050, Score: 0.20904\n",
      "Feature: 2051, Score: 0.16873\n",
      "Feature: 2052, Score: -0.41564\n",
      "Feature: 2053, Score: -0.15061\n",
      "Feature: 2054, Score: 0.20904\n",
      "Feature: 2055, Score: -0.34936\n",
      "Feature: 2056, Score: -0.26894\n",
      "Feature: 2057, Score: -0.37802\n",
      "Feature: 2058, Score: -0.41564\n",
      "Feature: 2059, Score: -0.41564\n",
      "Feature: 2060, Score: -0.41564\n",
      "Feature: 2061, Score: -0.41564\n",
      "Feature: 2062, Score: -0.20467\n",
      "Feature: 2063, Score: -0.34936\n",
      "Feature: 2064, Score: -0.26894\n",
      "Feature: 2065, Score: 0.25243\n",
      "Feature: 2066, Score: -0.34936\n",
      "Feature: 2067, Score: -0.24269\n",
      "Feature: 2068, Score: -0.34936\n",
      "Feature: 2069, Score: 0.25243\n",
      "Feature: 2070, Score: 0.32593\n",
      "Feature: 2071, Score: 0.25243\n",
      "Feature: 2072, Score: -0.24269\n",
      "Feature: 2073, Score: -0.26894\n",
      "Feature: 2074, Score: -0.26894\n",
      "Feature: 2075, Score: 0.19323\n",
      "Feature: 2076, Score: 0.32593\n",
      "Feature: 2077, Score: -0.24269\n",
      "Feature: 2078, Score: -0.26894\n",
      "Feature: 2079, Score: -0.15061\n",
      "Feature: 2080, Score: -0.26894\n",
      "Feature: 2081, Score: 0.18000\n",
      "Feature: 2082, Score: -0.34936\n",
      "Feature: 2083, Score: -0.30304\n",
      "Feature: 2084, Score: 0.28366\n",
      "Feature: 2085, Score: 0.19323\n",
      "Feature: 2086, Score: 0.22831\n",
      "Feature: 2087, Score: 0.32593\n",
      "Feature: 2088, Score: -0.26894\n",
      "Feature: 2089, Score: 0.28366\n",
      "Feature: 2090, Score: -0.41564\n",
      "Feature: 2091, Score: -0.41564\n",
      "Feature: 2092, Score: -0.30304\n",
      "Feature: 2093, Score: -0.41564\n",
      "Feature: 2094, Score: -0.30304\n",
      "Feature: 2095, Score: 0.38637\n",
      "Feature: 2096, Score: -0.17824\n",
      "Feature: 2097, Score: -0.26894\n",
      "Feature: 2098, Score: 0.16873\n",
      "Feature: 2099, Score: 0.25243\n",
      "Feature: 2100, Score: -0.34936\n",
      "Feature: 2101, Score: -0.41564\n",
      "Feature: 2102, Score: -0.19038\n",
      "Feature: 2103, Score: -0.34936\n",
      "Feature: 2104, Score: -0.19038\n",
      "Feature: 2105, Score: -0.30304\n",
      "Feature: 2106, Score: -0.34936\n",
      "Feature: 2107, Score: -0.30304\n",
      "Feature: 2108, Score: -0.20467\n",
      "Feature: 2109, Score: 0.13633\n",
      "Feature: 2110, Score: -0.34936\n",
      "Feature: 2111, Score: 0.13633\n",
      "Feature: 2112, Score: -0.19038\n",
      "Feature: 2113, Score: -0.30304\n",
      "Feature: 2114, Score: -0.30304\n",
      "Feature: 2115, Score: 0.22831\n",
      "Feature: 2116, Score: 0.38637\n",
      "Feature: 2117, Score: 0.38637\n",
      "Feature: 2118, Score: -0.34936\n",
      "Feature: 2119, Score: -0.26894\n",
      "Feature: 2120, Score: -0.26894\n",
      "Feature: 2121, Score: 0.19323\n",
      "Feature: 2122, Score: -0.24269\n",
      "Feature: 2123, Score: -0.34936\n",
      "Feature: 2124, Score: 0.28366\n",
      "Feature: 2125, Score: 0.38637\n",
      "Feature: 2126, Score: 0.25243\n",
      "Feature: 2127, Score: -0.24269\n",
      "Feature: 2128, Score: 0.25243\n",
      "Feature: 2129, Score: -0.34936\n",
      "Feature: 2130, Score: -0.19038\n",
      "Feature: 2131, Score: 0.38637\n",
      "Feature: 2132, Score: 0.32593\n",
      "Feature: 2133, Score: 0.16873\n",
      "Feature: 2134, Score: -0.22178\n",
      "Feature: 2135, Score: -0.20467\n",
      "Feature: 2136, Score: -0.19038\n",
      "Feature: 2137, Score: -0.24269\n",
      "Feature: 2138, Score: 0.38637\n",
      "Feature: 2139, Score: -0.34936\n",
      "Feature: 2140, Score: -0.30304\n",
      "Feature: 2141, Score: -0.22178\n",
      "Feature: 2142, Score: -0.34936\n",
      "Feature: 2143, Score: -0.26894\n",
      "Feature: 2144, Score: -0.20467\n",
      "Feature: 2145, Score: -0.22178\n",
      "Feature: 2146, Score: -0.34936\n",
      "Feature: 2147, Score: -0.22178\n",
      "Feature: 2148, Score: 0.32593\n",
      "Feature: 2149, Score: -0.17824\n",
      "Feature: 2150, Score: 0.15900\n",
      "Feature: 2151, Score: -0.30304\n",
      "Feature: 2152, Score: -0.22178\n",
      "Feature: 2153, Score: 0.25243\n",
      "Feature: 2154, Score: 0.25243\n",
      "Feature: 2155, Score: 0.22831\n",
      "Feature: 2156, Score: 0.38637\n",
      "Feature: 2157, Score: -0.30304\n",
      "Feature: 2158, Score: -0.24269\n",
      "Feature: 2159, Score: -0.26894\n",
      "Feature: 2160, Score: 0.38637\n",
      "Feature: 2161, Score: 0.25243\n",
      "Feature: 2162, Score: -0.26894\n",
      "Feature: 2163, Score: 0.32593\n",
      "Feature: 2164, Score: -0.20467\n",
      "Feature: 2165, Score: -0.34936\n",
      "Feature: 2166, Score: 0.38637\n",
      "Feature: 2167, Score: -0.30304\n",
      "Feature: 2168, Score: -0.41564\n",
      "Feature: 2169, Score: 0.38637\n",
      "Feature: 2170, Score: 0.18000\n",
      "Feature: 2171, Score: 0.32593\n",
      "Feature: 2172, Score: -0.34936\n",
      "Feature: 2173, Score: -0.34936\n",
      "Feature: 2174, Score: -0.30304\n",
      "Feature: 2175, Score: 0.28366\n",
      "Feature: 2176, Score: 0.22831\n",
      "Feature: 2177, Score: -0.24269\n",
      "Feature: 2178, Score: -0.30304\n",
      "Feature: 2179, Score: -0.34936\n",
      "Feature: 2180, Score: 0.22831\n",
      "Feature: 2181, Score: 0.15900\n",
      "Feature: 2182, Score: 0.32593\n",
      "Feature: 2183, Score: 0.25243\n",
      "Feature: 2184, Score: -0.14346\n",
      "Feature: 2185, Score: -0.22178\n",
      "Feature: 2186, Score: -0.26894\n",
      "Feature: 2187, Score: -0.30304\n",
      "Feature: 2188, Score: 0.20904\n",
      "Feature: 2189, Score: 0.32593\n",
      "Feature: 2190, Score: 0.32593\n",
      "Feature: 2191, Score: -0.24269\n",
      "Feature: 2192, Score: 0.38637\n",
      "Feature: 2193, Score: -0.41564\n",
      "Feature: 2194, Score: -0.34936\n",
      "Feature: 2195, Score: -0.34936\n",
      "Feature: 2196, Score: -0.30304\n",
      "Feature: 2197, Score: -0.41564\n",
      "Feature: 2198, Score: 0.22831\n",
      "Feature: 2199, Score: -0.26894\n",
      "Feature: 2200, Score: -0.20467\n",
      "Feature: 2201, Score: -0.26894\n",
      "Feature: 2202, Score: -0.22178\n",
      "Feature: 2203, Score: 0.38637\n",
      "Feature: 2204, Score: -0.30304\n",
      "Feature: 2205, Score: 0.22831\n",
      "Feature: 2206, Score: -0.30304\n",
      "Feature: 2207, Score: 0.28366\n",
      "Feature: 2208, Score: 0.38637\n",
      "Feature: 2209, Score: 0.38637\n",
      "Feature: 2210, Score: 0.32593\n",
      "Feature: 2211, Score: -0.17824\n",
      "Feature: 2212, Score: -0.19038\n",
      "Feature: 2213, Score: 0.22831\n",
      "Feature: 2214, Score: -0.41564\n",
      "Feature: 2215, Score: -0.34936\n",
      "Feature: 2216, Score: 0.19323\n",
      "Feature: 2217, Score: -0.26894\n",
      "Feature: 2218, Score: 0.32593\n",
      "Feature: 2219, Score: -0.53788\n",
      "Feature: 2220, Score: 0.38637\n",
      "Feature: 2221, Score: -0.26894\n",
      "Feature: 2222, Score: -0.24269\n",
      "Feature: 2223, Score: 0.19323\n",
      "Feature: 2224, Score: 0.19323\n",
      "Feature: 2225, Score: -0.26894\n",
      "Feature: 2226, Score: -0.20467\n",
      "Feature: 2227, Score: -0.19038\n",
      "Feature: 2228, Score: -0.48539\n",
      "Feature: 2229, Score: -0.24269\n",
      "Feature: 2230, Score: 0.28366\n",
      "Feature: 2231, Score: -0.30304\n",
      "Feature: 2232, Score: 0.38637\n",
      "Feature: 2233, Score: 0.28366\n",
      "Feature: 2234, Score: -0.41564\n",
      "Feature: 2235, Score: 0.15900\n",
      "Feature: 2236, Score: -0.30304\n",
      "Feature: 2237, Score: -0.30304\n",
      "Feature: 2238, Score: -0.12601\n",
      "Feature: 2239, Score: -0.30304\n",
      "Feature: 2240, Score: -0.34936\n",
      "Feature: 2241, Score: -0.19038\n",
      "Feature: 2242, Score: 0.32593\n",
      "Feature: 2243, Score: -0.34936\n",
      "Feature: 2244, Score: -0.34936\n",
      "Feature: 2245, Score: -0.30304\n",
      "Feature: 2246, Score: 0.19323\n",
      "Feature: 2247, Score: -0.26894\n",
      "Feature: 2248, Score: -0.20467\n",
      "Feature: 2249, Score: 0.38637\n",
      "Feature: 2250, Score: -0.22178\n",
      "Feature: 2251, Score: -0.30304\n",
      "Feature: 2252, Score: -0.24269\n",
      "Feature: 2253, Score: 0.32593\n",
      "Feature: 2254, Score: -0.13705\n",
      "Feature: 2255, Score: -0.30304\n",
      "Feature: 2256, Score: 0.32593\n",
      "Feature: 2257, Score: 0.28366\n",
      "Feature: 2258, Score: -0.13126\n",
      "Feature: 2259, Score: -0.26894\n",
      "Feature: 2260, Score: 0.32593\n",
      "Feature: 2261, Score: -0.15061\n",
      "Feature: 2262, Score: -0.34936\n",
      "Feature: 2263, Score: -0.41564\n",
      "Feature: 2264, Score: 0.45662\n",
      "Feature: 2265, Score: 0.14301\n",
      "Feature: 2266, Score: -0.15061\n",
      "Feature: 2267, Score: -0.41564\n",
      "Feature: 2268, Score: 0.10755\n",
      "Feature: 2269, Score: 0.28366\n",
      "Feature: 2270, Score: 0.19323\n",
      "Feature: 2271, Score: 0.20904\n",
      "Feature: 2272, Score: -0.24269\n",
      "Feature: 2273, Score: 0.32593\n",
      "Feature: 2274, Score: 0.15900\n",
      "Feature: 2275, Score: 0.15900\n",
      "Feature: 2276, Score: 0.22831\n",
      "Feature: 2277, Score: 0.32593\n",
      "Feature: 2278, Score: 0.32593\n",
      "Feature: 2279, Score: 0.25243\n",
      "Feature: 2280, Score: 0.32593\n",
      "Feature: 2281, Score: 0.22831\n",
      "Feature: 2282, Score: 0.15900\n",
      "Feature: 2283, Score: 0.28366\n",
      "Feature: 2284, Score: 0.32593\n",
      "Feature: 2285, Score: 0.38637\n",
      "Feature: 2286, Score: 0.16873\n",
      "Feature: 2287, Score: -0.20467\n",
      "Feature: 2288, Score: -0.41564\n",
      "Feature: 2289, Score: 0.38637\n",
      "Feature: 2290, Score: 0.38637\n",
      "Feature: 2291, Score: -0.34936\n",
      "Feature: 2292, Score: -0.30304\n",
      "Feature: 2293, Score: 0.25243\n",
      "Feature: 2294, Score: -0.34936\n",
      "Feature: 2295, Score: -0.20467\n",
      "Feature: 2296, Score: -0.34936\n",
      "Feature: 2297, Score: 0.10755\n",
      "Feature: 2298, Score: 0.38637\n",
      "Feature: 2299, Score: -0.13126\n",
      "Feature: 2300, Score: -0.41564\n",
      "Feature: 2301, Score: -0.26894\n",
      "Feature: 2302, Score: -0.30304\n",
      "Feature: 2303, Score: 0.39101\n",
      "Feature: 2304, Score: 0.32593\n",
      "Feature: 2305, Score: -0.26894\n",
      "Feature: 2306, Score: -0.41564\n",
      "Feature: 2307, Score: -0.17824\n",
      "Feature: 2308, Score: -0.30304\n",
      "Feature: 2309, Score: -0.41564\n",
      "Feature: 2310, Score: 0.16873\n",
      "Feature: 2311, Score: -0.20467\n",
      "Feature: 2312, Score: 0.32593\n",
      "Feature: 2313, Score: -0.44356\n",
      "Feature: 2314, Score: 0.38637\n",
      "Feature: 2315, Score: -0.30304\n",
      "Feature: 2316, Score: -0.19038\n",
      "Feature: 2317, Score: -0.41564\n",
      "Feature: 2318, Score: -0.44356\n",
      "Feature: 2319, Score: -0.24269\n",
      "Feature: 2320, Score: 0.32593\n",
      "Feature: 2321, Score: 0.25243\n",
      "Feature: 2322, Score: -0.41564\n",
      "Feature: 2323, Score: -0.41564\n",
      "Feature: 2324, Score: 0.32593\n",
      "Feature: 2325, Score: -0.19038\n",
      "Feature: 2326, Score: -0.26894\n",
      "Feature: 2327, Score: -0.34936\n",
      "Feature: 2328, Score: -0.30304\n",
      "Feature: 2329, Score: -0.34936\n",
      "Feature: 2330, Score: -0.19038\n",
      "Feature: 2331, Score: -0.24269\n",
      "Feature: 2332, Score: 0.38637\n",
      "Feature: 2333, Score: -0.41564\n",
      "Feature: 2334, Score: -0.30304\n",
      "Feature: 2335, Score: 0.28366\n",
      "Feature: 2336, Score: -0.34936\n",
      "Feature: 2337, Score: 0.38637\n",
      "Feature: 2338, Score: -0.22178\n",
      "Feature: 2339, Score: -0.26894\n",
      "Feature: 2340, Score: 0.38637\n",
      "Feature: 2341, Score: 0.38637\n",
      "Feature: 2342, Score: 0.28366\n",
      "Feature: 2343, Score: 0.38637\n",
      "Feature: 2344, Score: -0.30304\n",
      "Feature: 2345, Score: -0.13705\n",
      "Feature: 2346, Score: 0.22831\n",
      "Feature: 2347, Score: -0.41564\n",
      "Feature: 2348, Score: 0.38637\n",
      "Feature: 2349, Score: 0.22831\n",
      "Feature: 2350, Score: -0.24269\n",
      "Feature: 2351, Score: 0.22831\n",
      "Feature: 2352, Score: -0.41564\n",
      "Feature: 2353, Score: -0.26894\n",
      "Feature: 2354, Score: 0.25243\n",
      "Feature: 2355, Score: 0.19323\n",
      "Feature: 2356, Score: -0.30304\n",
      "Feature: 2357, Score: -0.19038\n",
      "Feature: 2358, Score: 0.32593\n",
      "Feature: 2359, Score: -0.24269\n",
      "Feature: 2360, Score: 0.28366\n",
      "Feature: 2361, Score: 0.15900\n",
      "Feature: 2362, Score: -0.24269\n",
      "Feature: 2363, Score: -0.41564\n",
      "Feature: 2364, Score: 0.32593\n",
      "Feature: 2365, Score: -0.30304\n",
      "Feature: 2366, Score: -0.22178\n",
      "Feature: 2367, Score: -0.24269\n",
      "Feature: 2368, Score: -0.41564\n",
      "Feature: 2369, Score: 0.32593\n",
      "Feature: 2370, Score: -0.26894\n",
      "Feature: 2371, Score: 0.20904\n",
      "Feature: 2372, Score: -0.38076\n",
      "Feature: 2373, Score: 0.25243\n",
      "Feature: 2374, Score: 0.38637\n",
      "Feature: 2375, Score: -0.30304\n",
      "Feature: 2376, Score: 0.22831\n",
      "Feature: 2377, Score: -0.24269\n",
      "Feature: 2378, Score: -0.34936\n",
      "Feature: 2379, Score: -0.34936\n",
      "Feature: 2380, Score: -0.30304\n",
      "Feature: 2381, Score: -0.34936\n",
      "Feature: 2382, Score: -0.34936\n",
      "Feature: 2383, Score: 0.22831\n",
      "Feature: 2384, Score: -0.34936\n",
      "Feature: 2385, Score: -0.24269\n",
      "Feature: 2386, Score: 0.25243\n",
      "Feature: 2387, Score: -0.34936\n",
      "Feature: 2388, Score: 0.28366\n",
      "Feature: 2389, Score: -0.34936\n",
      "Feature: 2390, Score: 0.18000\n",
      "Feature: 2391, Score: -0.30304\n",
      "Feature: 2392, Score: -0.30304\n",
      "Feature: 2393, Score: 0.38637\n",
      "Feature: 2394, Score: -0.34936\n",
      "Feature: 2395, Score: 0.32593\n",
      "Feature: 2396, Score: -0.26894\n",
      "Feature: 2397, Score: -0.19038\n",
      "Feature: 2398, Score: 0.19323\n",
      "Feature: 2399, Score: -0.41564\n",
      "Feature: 2400, Score: -0.24269\n",
      "Feature: 2401, Score: -0.34936\n",
      "Feature: 2402, Score: -0.30304\n",
      "Feature: 2403, Score: -0.34936\n",
      "Feature: 2404, Score: 0.38637\n",
      "Feature: 2405, Score: -0.30304\n",
      "Feature: 2406, Score: -0.41564\n",
      "Feature: 2407, Score: -0.41564\n",
      "Feature: 2408, Score: 0.32593\n",
      "Feature: 2409, Score: -0.15061\n",
      "Feature: 2410, Score: 0.32593\n",
      "Feature: 2411, Score: 0.38637\n",
      "Feature: 2412, Score: 0.28366\n",
      "Feature: 2413, Score: -0.41564\n",
      "Feature: 2414, Score: -0.20467\n",
      "Feature: 2415, Score: 0.32593\n",
      "Feature: 2416, Score: -0.48539\n",
      "Feature: 2417, Score: 0.22831\n",
      "Feature: 2418, Score: 0.19323\n",
      "Feature: 2419, Score: 0.25243\n",
      "Feature: 2420, Score: -0.30304\n",
      "Feature: 2421, Score: -0.26252\n",
      "Feature: 2422, Score: -0.34936\n",
      "Feature: 2423, Score: -0.26894\n",
      "Feature: 2424, Score: -0.20467\n",
      "Feature: 2425, Score: -0.34936\n",
      "Feature: 2426, Score: -0.30304\n",
      "Feature: 2427, Score: -0.30304\n",
      "Feature: 2428, Score: -0.34936\n",
      "Feature: 2429, Score: -0.41564\n",
      "Feature: 2430, Score: -0.26894\n",
      "Feature: 2431, Score: -0.34936\n",
      "Feature: 2432, Score: -0.41564\n",
      "Feature: 2433, Score: -0.41564\n",
      "Feature: 2434, Score: 0.31801\n",
      "Feature: 2435, Score: -0.34936\n",
      "Feature: 2436, Score: 0.38637\n",
      "Feature: 2437, Score: 0.32593\n",
      "Feature: 2438, Score: -0.41564\n",
      "Feature: 2439, Score: -0.24269\n",
      "Feature: 2440, Score: -0.30304\n",
      "Feature: 2441, Score: -0.41564\n",
      "Feature: 2442, Score: -0.24269\n",
      "Feature: 2443, Score: -0.19038\n",
      "Feature: 2444, Score: 0.32593\n",
      "Feature: 2445, Score: -0.30304\n",
      "Feature: 2446, Score: 0.25243\n",
      "Feature: 2447, Score: -0.26894\n",
      "Feature: 2448, Score: -0.41564\n",
      "Feature: 2449, Score: -0.53788\n",
      "Feature: 2450, Score: -0.34936\n",
      "Feature: 2451, Score: 0.15900\n",
      "Feature: 2452, Score: 0.25243\n",
      "Feature: 2453, Score: -0.30304\n",
      "Feature: 2454, Score: -0.34936\n",
      "Feature: 2455, Score: -0.30304\n",
      "Feature: 2456, Score: 0.20904\n",
      "Feature: 2457, Score: -0.28692\n",
      "Feature: 2458, Score: -0.41564\n",
      "Feature: 2459, Score: 0.25243\n",
      "Feature: 2460, Score: -0.26894\n",
      "Feature: 2461, Score: -0.26894\n",
      "Feature: 2462, Score: 0.25243\n",
      "Feature: 2463, Score: 0.36000\n",
      "Feature: 2464, Score: -0.22178\n",
      "Feature: 2465, Score: -0.41564\n",
      "Feature: 2466, Score: -0.26894\n",
      "Feature: 2467, Score: 0.32593\n",
      "Feature: 2468, Score: -0.30304\n",
      "Feature: 2469, Score: -0.34936\n",
      "Feature: 2470, Score: -0.24269\n",
      "Feature: 2471, Score: -0.41564\n",
      "Feature: 2472, Score: -0.34936\n",
      "Feature: 2473, Score: -0.19038\n",
      "Feature: 2474, Score: -0.41564\n",
      "Feature: 2475, Score: -0.34936\n",
      "Feature: 2476, Score: -0.26894\n",
      "Feature: 2477, Score: -0.34936\n",
      "Feature: 2478, Score: 0.25243\n",
      "Feature: 2479, Score: -0.20467\n",
      "Feature: 2480, Score: -0.22178\n",
      "Feature: 2481, Score: -0.41564\n",
      "Feature: 2482, Score: 0.38637\n",
      "Feature: 2483, Score: -0.30304\n",
      "Feature: 2484, Score: -0.19038\n",
      "Feature: 2485, Score: -0.30304\n",
      "Feature: 2486, Score: 0.25243\n",
      "Feature: 2487, Score: -0.30304\n",
      "Feature: 2488, Score: -0.22178\n",
      "Feature: 2489, Score: -0.24269\n",
      "Feature: 2490, Score: 0.38637\n",
      "Feature: 2491, Score: 0.22831\n",
      "Feature: 2492, Score: -0.26894\n",
      "Feature: 2493, Score: 0.32593\n",
      "Feature: 2494, Score: 0.38637\n",
      "Feature: 2495, Score: 0.20904\n",
      "Feature: 2496, Score: 0.20904\n",
      "Feature: 2497, Score: -0.41564\n",
      "Feature: 2498, Score: -0.30304\n",
      "Feature: 2499, Score: -0.22178\n",
      "Feature: 2500, Score: 0.22831\n",
      "Feature: 2501, Score: -0.15061\n",
      "Feature: 2502, Score: 0.38637\n",
      "Feature: 2503, Score: 0.38637\n",
      "Feature: 2504, Score: -0.41564\n",
      "Feature: 2505, Score: -0.24269\n",
      "Feature: 2506, Score: -0.24269\n",
      "Feature: 2507, Score: -0.24269\n",
      "Feature: 2508, Score: -0.34936\n",
      "Feature: 2509, Score: 0.14301\n",
      "Feature: 2510, Score: 0.38637\n",
      "Feature: 2511, Score: 0.25243\n",
      "Feature: 2512, Score: -0.34936\n",
      "Feature: 2513, Score: 0.28366\n",
      "Feature: 2514, Score: -0.34936\n",
      "Feature: 2515, Score: 0.38637\n",
      "Feature: 2516, Score: 0.20904\n",
      "Feature: 2517, Score: 0.20904\n",
      "Feature: 2518, Score: -0.34936\n",
      "Feature: 2519, Score: -0.19038\n",
      "Feature: 2520, Score: -0.26894\n",
      "Feature: 2521, Score: -0.19038\n",
      "Feature: 2522, Score: -0.19038\n",
      "Feature: 2523, Score: -0.34936\n",
      "Feature: 2524, Score: 0.18000\n",
      "Feature: 2525, Score: 0.18000\n",
      "Feature: 2526, Score: 0.50486\n",
      "Feature: 2527, Score: -0.24269\n",
      "Feature: 2528, Score: -0.30304\n",
      "Feature: 2529, Score: 0.32593\n",
      "Feature: 2530, Score: -0.34936\n",
      "Feature: 2531, Score: -0.34936\n",
      "Feature: 2532, Score: -0.41564\n",
      "Feature: 2533, Score: 0.32593\n",
      "Feature: 2534, Score: 0.22831\n",
      "Feature: 2535, Score: 0.32593\n",
      "Feature: 2536, Score: -0.41564\n",
      "Feature: 2537, Score: -0.34936\n",
      "Feature: 2538, Score: -0.26894\n",
      "Feature: 2539, Score: -0.26894\n",
      "Feature: 2540, Score: 0.28366\n",
      "Feature: 2541, Score: -0.20467\n",
      "Feature: 2542, Score: -0.30304\n",
      "Feature: 2543, Score: -0.30304\n",
      "Feature: 2544, Score: 0.22831\n",
      "Feature: 2545, Score: -0.20467\n",
      "Feature: 2546, Score: -0.41564\n",
      "Feature: 2547, Score: -0.30304\n",
      "Feature: 2548, Score: -0.41564\n",
      "Feature: 2549, Score: 0.38637\n",
      "Feature: 2550, Score: 0.28366\n",
      "Feature: 2551, Score: 0.38637\n",
      "Feature: 2552, Score: -0.26894\n",
      "Feature: 2553, Score: -0.34936\n",
      "Feature: 2554, Score: -0.41564\n",
      "Feature: 2555, Score: 0.25243\n",
      "Feature: 2556, Score: -0.22178\n",
      "Feature: 2557, Score: 0.32593\n",
      "Feature: 2558, Score: -0.22178\n",
      "Feature: 2559, Score: -0.24269\n",
      "Feature: 2560, Score: -0.30304\n",
      "Feature: 2561, Score: -0.30304\n",
      "Feature: 2562, Score: 0.10755\n",
      "Feature: 2563, Score: 0.22831\n",
      "Feature: 2564, Score: -0.19038\n",
      "Feature: 2565, Score: -0.19038\n",
      "Feature: 2566, Score: 0.15900\n",
      "Feature: 2567, Score: -0.26894\n",
      "Feature: 2568, Score: -0.34936\n",
      "Feature: 2569, Score: 0.38637\n",
      "Feature: 2570, Score: -0.20467\n",
      "Feature: 2571, Score: 0.32593\n",
      "Feature: 2572, Score: -0.34936\n",
      "Feature: 2573, Score: -0.30304\n",
      "Feature: 2574, Score: 0.38637\n",
      "Feature: 2575, Score: 0.09769\n",
      "Feature: 2576, Score: 0.28366\n",
      "Feature: 2577, Score: -0.15061\n",
      "Feature: 2578, Score: 0.22831\n",
      "Feature: 2579, Score: -0.13705\n",
      "Feature: 2580, Score: 0.19323\n",
      "Feature: 2581, Score: -0.12601\n",
      "Feature: 2582, Score: -0.41564\n",
      "Feature: 2583, Score: -0.19038\n",
      "Feature: 2584, Score: -0.19038\n",
      "Feature: 2585, Score: -0.15061\n",
      "Feature: 2586, Score: -0.24269\n",
      "Feature: 2587, Score: -0.30304\n",
      "Feature: 2588, Score: 0.19323\n",
      "Feature: 2589, Score: -0.41564\n",
      "Feature: 2590, Score: 0.16873\n",
      "Feature: 2591, Score: -0.24269\n",
      "Feature: 2592, Score: 0.38637\n",
      "Feature: 2593, Score: -0.34936\n",
      "Feature: 2594, Score: -0.30304\n",
      "Feature: 2595, Score: -0.41564\n",
      "Feature: 2596, Score: 0.20904\n",
      "Feature: 2597, Score: -0.22178\n",
      "Feature: 2598, Score: 0.28366\n",
      "Feature: 2599, Score: 0.32593\n",
      "Feature: 2600, Score: 0.32593\n",
      "Feature: 2601, Score: -0.13126\n",
      "Feature: 2602, Score: -0.34936\n",
      "Feature: 2603, Score: 0.25243\n",
      "Feature: 2604, Score: -0.34936\n",
      "Feature: 2605, Score: -0.41564\n",
      "Feature: 2606, Score: 0.22831\n",
      "Feature: 2607, Score: -0.30304\n",
      "Feature: 2608, Score: 0.38637\n",
      "Feature: 2609, Score: -0.22178\n",
      "Feature: 2610, Score: 0.15900\n",
      "Feature: 2611, Score: 0.32593\n",
      "Feature: 2612, Score: 0.19323\n",
      "Feature: 2613, Score: -0.30304\n",
      "Feature: 2614, Score: -0.24269\n",
      "Feature: 2615, Score: -0.34936\n",
      "Feature: 2616, Score: -0.30304\n",
      "Feature: 2617, Score: -0.22178\n",
      "Feature: 2618, Score: 0.28366\n",
      "Feature: 2619, Score: 0.19323\n",
      "Feature: 2620, Score: -0.20467\n",
      "Feature: 2621, Score: -0.41564\n",
      "Feature: 2622, Score: -0.34936\n",
      "Feature: 2623, Score: 0.32593\n",
      "Feature: 2624, Score: 0.38637\n",
      "Feature: 2625, Score: -0.41564\n",
      "Feature: 2626, Score: -0.20467\n",
      "Feature: 2627, Score: 0.28366\n",
      "Feature: 2628, Score: -0.30304\n",
      "Feature: 2629, Score: -0.41564\n",
      "Feature: 2630, Score: -0.44356\n",
      "Feature: 2631, Score: 0.38637\n",
      "Feature: 2632, Score: -0.13705\n",
      "Feature: 2633, Score: -0.24269\n",
      "Feature: 2634, Score: 0.18000\n",
      "Feature: 2635, Score: 0.28366\n",
      "Feature: 2636, Score: -0.34936\n",
      "Feature: 2637, Score: 0.38637\n",
      "Feature: 2638, Score: 0.32593\n",
      "Feature: 2639, Score: -0.13705\n",
      "Feature: 2640, Score: -0.20467\n",
      "Feature: 2641, Score: -0.19038\n",
      "Feature: 2642, Score: 0.38637\n",
      "Feature: 2643, Score: -0.30304\n",
      "Feature: 2644, Score: -0.22178\n",
      "Feature: 2645, Score: -0.19038\n",
      "Feature: 2646, Score: 0.18000\n",
      "Feature: 2647, Score: -0.26894\n",
      "Feature: 2648, Score: 0.32593\n",
      "Feature: 2649, Score: 0.15900\n",
      "Feature: 2650, Score: -0.30304\n",
      "Feature: 2651, Score: 0.20904\n",
      "Feature: 2652, Score: -0.19038\n",
      "Feature: 2653, Score: 0.16873\n",
      "Feature: 2654, Score: -0.30304\n",
      "Feature: 2655, Score: 0.38637\n",
      "Feature: 2656, Score: 0.28366\n",
      "Feature: 2657, Score: 0.18000\n",
      "Feature: 2658, Score: -0.34936\n",
      "Feature: 2659, Score: -0.19038\n",
      "Feature: 2660, Score: -0.41564\n",
      "Feature: 2661, Score: -0.19038\n",
      "Feature: 2662, Score: 0.28366\n",
      "Feature: 2663, Score: 0.28366\n",
      "Feature: 2664, Score: 0.14301\n",
      "Feature: 2665, Score: 0.28366\n",
      "Feature: 2666, Score: -0.30304\n",
      "Feature: 2667, Score: 0.13034\n",
      "Feature: 2668, Score: -0.41564\n",
      "Feature: 2669, Score: 0.28366\n",
      "Feature: 2670, Score: 0.38637\n",
      "Feature: 2671, Score: 0.28366\n",
      "Feature: 2672, Score: -0.15865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 2673, Score: 0.20904\n",
      "Feature: 2674, Score: -0.34936\n",
      "Feature: 2675, Score: 0.38637\n",
      "Feature: 2676, Score: 0.32593\n",
      "Feature: 2677, Score: -0.41564\n",
      "Feature: 2678, Score: 0.32593\n",
      "Feature: 2679, Score: -0.30304\n",
      "Feature: 2680, Score: -0.41564\n",
      "Feature: 2681, Score: -0.15865\n",
      "Feature: 2682, Score: -0.30304\n",
      "Feature: 2683, Score: 0.38637\n",
      "Feature: 2684, Score: -0.24269\n",
      "Feature: 2685, Score: 0.15900\n",
      "Feature: 2686, Score: -0.41564\n",
      "Feature: 2687, Score: -0.34936\n",
      "Feature: 2688, Score: 0.32593\n",
      "Feature: 2689, Score: -0.41564\n",
      "Feature: 2690, Score: 0.19323\n",
      "Feature: 2691, Score: -0.20467\n",
      "Feature: 2692, Score: -0.20467\n",
      "Feature: 2693, Score: -0.19038\n",
      "Feature: 2694, Score: -0.34936\n",
      "Feature: 2695, Score: -0.34936\n",
      "Feature: 2696, Score: 0.25243\n",
      "Feature: 2697, Score: 0.28366\n",
      "Feature: 2698, Score: -0.19038\n",
      "Feature: 2699, Score: -0.30304\n",
      "Feature: 2700, Score: -0.34936\n",
      "Feature: 2701, Score: -0.24269\n",
      "Feature: 2702, Score: -0.30304\n",
      "Feature: 2703, Score: 0.18000\n",
      "Feature: 2704, Score: 0.15900\n",
      "Feature: 2705, Score: 0.32593\n",
      "Feature: 2706, Score: 0.32593\n",
      "Feature: 2707, Score: 0.38637\n",
      "Feature: 2708, Score: -0.34936\n",
      "Feature: 2709, Score: -0.13705\n",
      "Feature: 2710, Score: -0.30304\n",
      "Feature: 2711, Score: 0.38637\n",
      "Feature: 2712, Score: 0.16873\n",
      "Feature: 2713, Score: -0.34936\n",
      "Feature: 2714, Score: -0.34936\n",
      "Feature: 2715, Score: 0.28366\n",
      "Feature: 2716, Score: -0.34936\n",
      "Feature: 2717, Score: 0.38637\n",
      "Feature: 2718, Score: -0.41564\n",
      "Feature: 2719, Score: 0.38637\n",
      "Feature: 2720, Score: 0.38637\n",
      "Feature: 2721, Score: -0.19038\n",
      "Feature: 2722, Score: -0.19038\n",
      "Feature: 2723, Score: 0.13034\n",
      "Feature: 2724, Score: -0.19038\n",
      "Feature: 2725, Score: -0.30304\n",
      "Feature: 2726, Score: -0.26894\n",
      "Feature: 2727, Score: -0.41564\n",
      "Feature: 2728, Score: -0.30304\n",
      "Feature: 2729, Score: 0.38637\n",
      "Feature: 2730, Score: -0.41564\n",
      "Feature: 2731, Score: 0.38637\n",
      "Feature: 2732, Score: 0.32593\n",
      "Feature: 2733, Score: 0.16873\n",
      "Feature: 2734, Score: 0.38637\n",
      "Feature: 2735, Score: -0.41564\n",
      "Feature: 2736, Score: -0.20467\n",
      "Feature: 2737, Score: 0.38637\n",
      "Feature: 2738, Score: 0.16873\n",
      "Feature: 2739, Score: 0.13633\n",
      "Feature: 2740, Score: -0.15061\n",
      "Feature: 2741, Score: 0.38637\n",
      "Feature: 2742, Score: 0.32593\n",
      "Feature: 2743, Score: 0.32593\n",
      "Feature: 2744, Score: 0.32593\n",
      "Feature: 2745, Score: -0.34936\n",
      "Feature: 2746, Score: -0.15061\n",
      "Feature: 2747, Score: -0.26252\n",
      "Feature: 2748, Score: -0.30304\n",
      "Feature: 2749, Score: 0.22831\n",
      "Feature: 2750, Score: 0.22831\n",
      "Feature: 2751, Score: -0.30304\n",
      "Feature: 2752, Score: -0.13126\n",
      "Feature: 2753, Score: 0.28366\n",
      "Feature: 2754, Score: -0.20467\n",
      "Feature: 2755, Score: -0.20467\n",
      "Feature: 2756, Score: -0.20467\n",
      "Feature: 2757, Score: -0.20467\n",
      "Feature: 2758, Score: 0.28366\n",
      "Feature: 2759, Score: -0.30304\n",
      "Feature: 2760, Score: -0.30304\n",
      "Feature: 2761, Score: 0.38637\n",
      "Feature: 2762, Score: 0.16873\n",
      "Feature: 2763, Score: 0.18000\n",
      "Feature: 2764, Score: 0.38637\n",
      "Feature: 2765, Score: 0.19323\n",
      "Feature: 2766, Score: -0.20467\n",
      "Feature: 2767, Score: -0.34936\n",
      "Feature: 2768, Score: -0.24269\n",
      "Feature: 2769, Score: -0.19038\n",
      "Feature: 2770, Score: -0.17824\n",
      "Feature: 2771, Score: -0.34936\n",
      "Feature: 2772, Score: 0.14301\n",
      "Feature: 2773, Score: -0.24269\n",
      "Feature: 2774, Score: 0.25243\n",
      "Feature: 2775, Score: -0.15865\n",
      "Feature: 2776, Score: -0.15865\n",
      "Feature: 2777, Score: -0.26894\n",
      "Feature: 2778, Score: -0.24269\n",
      "Feature: 2779, Score: -0.24269\n",
      "Feature: 2780, Score: -0.30304\n",
      "Feature: 2781, Score: -0.41564\n",
      "Feature: 2782, Score: 0.22831\n",
      "Feature: 2783, Score: 0.25243\n",
      "Feature: 2784, Score: -0.30304\n",
      "Feature: 2785, Score: 0.38637\n",
      "Feature: 2786, Score: -0.26894\n",
      "Feature: 2787, Score: 0.32593\n",
      "Feature: 2788, Score: -0.26894\n",
      "Feature: 2789, Score: -0.24269\n",
      "Feature: 2790, Score: -0.26894\n",
      "Feature: 2791, Score: 0.20904\n",
      "Feature: 2792, Score: 0.28366\n",
      "Feature: 2793, Score: -0.34936\n",
      "Feature: 2794, Score: -0.26894\n",
      "Feature: 2795, Score: 0.32593\n",
      "Feature: 2796, Score: 0.32593\n",
      "Feature: 2797, Score: -0.20467\n",
      "Feature: 2798, Score: 0.19323\n",
      "Feature: 2799, Score: 0.28366\n",
      "Feature: 2800, Score: -0.15061\n",
      "Feature: 2801, Score: -0.30304\n",
      "Feature: 2802, Score: 0.15900\n",
      "Feature: 2803, Score: 0.16873\n",
      "Feature: 2804, Score: 0.32593\n",
      "Feature: 2805, Score: 0.32593\n",
      "Feature: 2806, Score: -0.22178\n",
      "Feature: 2807, Score: -0.26894\n",
      "Feature: 2808, Score: 0.22831\n",
      "Feature: 2809, Score: 0.38637\n",
      "Feature: 2810, Score: -0.38076\n",
      "Feature: 2811, Score: 0.25243\n",
      "Feature: 2812, Score: 0.18000\n",
      "Feature: 2813, Score: 0.25243\n",
      "Feature: 2814, Score: 0.25243\n",
      "Feature: 2815, Score: -0.22178\n",
      "Feature: 2816, Score: 0.20904\n",
      "Feature: 2817, Score: 0.38637\n",
      "Feature: 2818, Score: 0.25243\n",
      "Feature: 2819, Score: -0.34936\n",
      "Feature: 2820, Score: -0.34936\n",
      "Feature: 2821, Score: -0.26894\n",
      "Feature: 2822, Score: 0.38637\n",
      "Feature: 2823, Score: 0.32593\n",
      "Feature: 2824, Score: -0.26894\n",
      "Feature: 2825, Score: -0.26894\n",
      "Feature: 2826, Score: 0.38637\n",
      "Feature: 2827, Score: 0.38637\n",
      "Feature: 2828, Score: -0.24269\n",
      "Feature: 2829, Score: 0.28366\n",
      "Feature: 2830, Score: 0.13034\n",
      "Feature: 2831, Score: -0.30304\n",
      "Feature: 2832, Score: 0.25243\n",
      "Feature: 2833, Score: 0.32593\n",
      "Feature: 2834, Score: 0.38637\n",
      "Feature: 2835, Score: -0.34936\n",
      "Feature: 2836, Score: -0.30304\n",
      "Feature: 2837, Score: -0.24269\n",
      "Feature: 2838, Score: -0.30304\n",
      "Feature: 2839, Score: -0.34936\n",
      "Feature: 2840, Score: 0.32593\n",
      "Feature: 2841, Score: 0.32593\n",
      "Feature: 2842, Score: 0.20904\n",
      "Feature: 2843, Score: 0.38637\n",
      "Feature: 2844, Score: -0.34936\n",
      "Feature: 2845, Score: 0.32593\n",
      "Feature: 2846, Score: -0.41564\n",
      "Feature: 2847, Score: 0.38637\n",
      "Feature: 2848, Score: -0.30304\n",
      "Feature: 2849, Score: -0.34936\n",
      "Feature: 2850, Score: -0.30304\n",
      "Feature: 2851, Score: 0.38637\n",
      "Feature: 2852, Score: -0.15061\n",
      "Feature: 2853, Score: 0.32593\n",
      "Feature: 2854, Score: 0.32593\n",
      "Feature: 2855, Score: 0.38637\n",
      "Feature: 2856, Score: 0.38637\n",
      "Feature: 2857, Score: 0.38637\n",
      "Feature: 2858, Score: 0.15051\n",
      "Feature: 2859, Score: 0.32593\n",
      "Feature: 2860, Score: -0.20467\n",
      "Feature: 2861, Score: 0.38637\n",
      "Feature: 2862, Score: 0.38637\n",
      "Feature: 2863, Score: 0.32593\n",
      "Feature: 2864, Score: -0.22178\n",
      "Feature: 2865, Score: 0.32593\n",
      "Feature: 2866, Score: -0.22178\n",
      "Feature: 2867, Score: 0.13633\n",
      "Feature: 2868, Score: -0.34936\n",
      "Feature: 2869, Score: 0.32593\n",
      "Feature: 2870, Score: -0.34936\n",
      "Feature: 2871, Score: -0.34936\n",
      "Feature: 2872, Score: 0.28366\n",
      "Feature: 2873, Score: -0.24269\n",
      "Feature: 2874, Score: 0.32593\n",
      "Feature: 2875, Score: -0.19038\n",
      "Feature: 2876, Score: -0.22178\n",
      "Feature: 2877, Score: -0.26894\n",
      "Feature: 2878, Score: -0.34936\n",
      "Feature: 2879, Score: 0.22831\n",
      "Feature: 2880, Score: 0.38637\n",
      "Feature: 2881, Score: 0.19323\n",
      "Feature: 2882, Score: -0.40934\n",
      "Feature: 2883, Score: 0.38637\n",
      "Feature: 2884, Score: 0.38637\n",
      "Feature: 2885, Score: 0.38637\n",
      "Feature: 2886, Score: -0.13705\n",
      "Feature: 2887, Score: -0.17824\n",
      "Feature: 2888, Score: -0.15061\n",
      "Feature: 2889, Score: -0.48539\n",
      "Feature: 2890, Score: -0.24269\n",
      "Feature: 2891, Score: 0.38637\n",
      "Feature: 2892, Score: 0.22831\n",
      "Feature: 2893, Score: 0.32593\n",
      "Feature: 2894, Score: 0.22831\n",
      "Feature: 2895, Score: 0.32593\n",
      "Feature: 2896, Score: -0.34936\n",
      "Feature: 2897, Score: 0.38637\n",
      "Feature: 2898, Score: 0.28366\n",
      "Feature: 2899, Score: 0.22831\n",
      "Feature: 2900, Score: 0.38637\n",
      "Feature: 2901, Score: -0.41564\n",
      "Feature: 2902, Score: 0.38637\n",
      "Feature: 2903, Score: -0.34936\n",
      "Feature: 2904, Score: 0.22831\n",
      "Feature: 2905, Score: -0.13705\n",
      "Feature: 2906, Score: -0.15061\n",
      "Feature: 2907, Score: -0.15061\n",
      "Feature: 2908, Score: -0.15061\n",
      "Feature: 2909, Score: -0.15061\n",
      "Feature: 2910, Score: -0.15061\n",
      "Feature: 2911, Score: -0.15061\n",
      "Feature: 2912, Score: -0.15061\n",
      "Feature: 2913, Score: -0.15061\n",
      "Feature: 2914, Score: -0.15061\n",
      "Feature: 2915, Score: -0.15061\n",
      "Feature: 2916, Score: -0.15061\n",
      "Feature: 2917, Score: -0.15061\n",
      "Feature: 2918, Score: 0.18000\n",
      "Feature: 2919, Score: 0.18000\n",
      "Feature: 2920, Score: 0.18000\n",
      "Feature: 2921, Score: 0.18000\n",
      "Feature: 2922, Score: 0.18000\n",
      "Feature: 2923, Score: 0.18000\n",
      "Feature: 2924, Score: 0.18000\n",
      "Feature: 2925, Score: 0.38637\n",
      "Feature: 2926, Score: -0.07725\n",
      "Feature: 2927, Score: -0.07725\n",
      "Feature: 2928, Score: -0.07725\n",
      "Feature: 2929, Score: -0.07725\n",
      "Feature: 2930, Score: -0.07725\n",
      "Feature: 2931, Score: -0.07725\n",
      "Feature: 2932, Score: -0.07725\n",
      "Feature: 2933, Score: -0.07725\n",
      "Feature: 2934, Score: -0.07725\n",
      "Feature: 2935, Score: -0.07725\n",
      "Feature: 2936, Score: -0.07725\n",
      "Feature: 2937, Score: -0.07725\n",
      "Feature: 2938, Score: -0.07725\n",
      "Feature: 2939, Score: -0.07725\n",
      "Feature: 2940, Score: -0.07725\n",
      "Feature: 2941, Score: -0.07725\n",
      "Feature: 2942, Score: -0.07725\n",
      "Feature: 2943, Score: -0.07725\n",
      "Feature: 2944, Score: -0.07725\n",
      "Feature: 2945, Score: -0.07725\n",
      "Feature: 2946, Score: -0.07725\n",
      "Feature: 2947, Score: -0.07725\n",
      "Feature: 2948, Score: -0.07725\n",
      "Feature: 2949, Score: -0.07725\n",
      "Feature: 2950, Score: -0.07725\n",
      "Feature: 2951, Score: -0.07725\n",
      "Feature: 2952, Score: -0.07725\n",
      "Feature: 2953, Score: -0.07725\n",
      "Feature: 2954, Score: -0.07725\n",
      "Feature: 2955, Score: -0.15451\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = X_train_cvec\n",
    "y = y_train\n",
    "#= make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance);\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the average test solutions is 69.60%.\n"
     ]
    }
   ],
   "source": [
    "# Average the y_test_hat vectors to see if we can improve the score\n",
    "y_test_hat_agg = sum(df_scores['y_test_hat'])\n",
    "y_test_hat = np.round(y_test_hat_agg/df_scores.shape[1])\n",
    "print(\"The accuracy score of the average test solutions is {:.2f}%.\".format(accuracy_score(y_test,y_test_hat)*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['y'] = y_test.values\n",
    "X_test['y_hat'] = y_test_hat\n",
    "X_test['y_hat_agg'] = y_test_hat_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y_hat_agg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>Thus Woman in Twitter....</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>This dude is a god :O ingenious Spanish writer...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>The Gladiators - Pretenting 12' - A bit of gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>The male in question groomed multiple young wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>Like actually wtf is this. (Found in r/cringet...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>this post with nearly 175,000 likes on instagr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>Gotta love those empty threats. And the consis...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Acting Homeland Security Secretary Chad Wolf T...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Rant. Insert attention-grabbing headline here.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Trump has been banned from social media. If on...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>What type of feminism do you believe in? Comin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>TrollXChromosomes banned me for debunking a po...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Sometimes it divide into 3 streams.. bruh</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>Moment of Truth: Stalking Back (1993) Shanna R...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>Think my dad is proud of me? Xx</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Blog created to advocate female supremacy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Death threats and crude insults: this is what ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>VOLUNTEER TO HELP SAVE CHILDREN FROM GENITAL C...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Leave feminists bark, why bother?</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>The meme community only cares about men</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  y  y_hat  y_hat_agg\n",
       "1190                          Thus Woman in Twitter....  0    1.0          5\n",
       "1411  This dude is a god :O ingenious Spanish writer...  0    1.0          7\n",
       "644   The Gladiators - Pretenting 12' - A bit of gre...  0    1.0          6\n",
       "1763  The male in question groomed multiple young wo...  1    0.0          2\n",
       "1226  Like actually wtf is this. (Found in r/cringet...  0    1.0          6\n",
       "296   this post with nearly 175,000 likes on instagr...  0    1.0          7\n",
       "1665  Gotta love those empty threats. And the consis...  1    0.0          3\n",
       "746   Acting Homeland Security Secretary Chad Wolf T...  0    1.0          7\n",
       "291      Rant. Insert attention-grabbing headline here.  0    1.0          5\n",
       "781   Trump has been banned from social media. If on...  0    1.0          6\n",
       "2144  What type of feminism do you believe in? Comin...  1    0.0          4\n",
       "322   TrollXChromosomes banned me for debunking a po...  0    1.0          6\n",
       "1090          Sometimes it divide into 3 streams.. bruh  0    1.0          8\n",
       "1359  Moment of Truth: Stalking Back (1993) Shanna R...  0    1.0          8\n",
       "1001                    Think my dad is proud of me? Xx  0    1.0          8\n",
       "718          Blog created to advocate female supremacy.  0    1.0          8\n",
       "842   Death threats and crude insults: this is what ...  0    1.0          6\n",
       "1491  VOLUNTEER TO HELP SAVE CHILDREN FROM GENITAL C...  0    1.0          6\n",
       "521                   Leave feminists bark, why bother?  0    1.0          8\n",
       "1921            The meme community only cares about men  1    0.0          4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_incorrect = X_test[X_test[\"y\"] != X_test['y_hat']]\n",
    "X_test_incorrect.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Executive summary\n",
    "### The data\n",
    "For this project, I chose two sub-reddits to compare, r/Feminists  and r/MensRights. \n",
    "\n",
    "### Exploring the data\n",
    "Once I had scraped the data, I ran a number of initial tests on it to get some insight. I used a word cloud to illustrate the word used most often in each subreddit. I then ran a vectorizer on unigrams and made some bar charts comparing the usage of the most words. Even in the top words of titles it provides insight into the communties, as some of the top words for Mensrights were \"Accused\", \"Women\", and \"Rape\".\n",
    "\n",
    "### Modeling the problem\n",
    "I explored a range of models and hypertuned the parameters countless times in combination with the use different vectorizers such as Count Vectorizer and TFIDF.\n",
    "\n",
    "\n",
    "### Evaluate the model\n",
    "The models were either overfit, and performing badly or non over and still performing poorly. The baseline accuracy for my model was 50%, my best test and train score both sat at around 70% and were attained using a simple Count Vectorizer and Logistic Regression. After examining the misclassifications it appears that many of these errors are because title's aren't giving specific enough information for our model to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
